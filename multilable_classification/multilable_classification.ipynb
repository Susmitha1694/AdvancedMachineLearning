{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oxyDjwnf1ojY"
   },
   "source": [
    "# COMP47590: Advanced Machine Learning\n",
    "# Assignment 1: Multi-label Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DbAU9ScE1ojb"
   },
   "source": [
    "Names: Susmitha Tadavarthi, Shaurya Gogia\n",
    "\n",
    "Student Numbers: 19200996, 19200891"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N6atnapb1ojc"
   },
   "source": [
    "## Import Packages Etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "U5hjIOiY1ojd",
    "outputId": "cf4a90cd-6e2a-4362-920e-55670bece30f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import copy\n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn import linear_model\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_KBUgbU11oji"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UOBnHowH1ojm"
   },
   "source": [
    "## Task 0: Load the Yeast Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "colab_type": "code",
    "id": "ob7jueI_1ojn",
    "outputId": "083556ee-3759-4719-e332-b12f9c92f50c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Att1</th>\n",
       "      <th>Att2</th>\n",
       "      <th>Att3</th>\n",
       "      <th>Att4</th>\n",
       "      <th>Att5</th>\n",
       "      <th>Att6</th>\n",
       "      <th>Att7</th>\n",
       "      <th>Att8</th>\n",
       "      <th>Att9</th>\n",
       "      <th>Att10</th>\n",
       "      <th>Att11</th>\n",
       "      <th>Att12</th>\n",
       "      <th>Att13</th>\n",
       "      <th>Att14</th>\n",
       "      <th>Att15</th>\n",
       "      <th>Att16</th>\n",
       "      <th>Att17</th>\n",
       "      <th>Att18</th>\n",
       "      <th>Att19</th>\n",
       "      <th>Att20</th>\n",
       "      <th>Att21</th>\n",
       "      <th>Att22</th>\n",
       "      <th>Att23</th>\n",
       "      <th>Att24</th>\n",
       "      <th>Att25</th>\n",
       "      <th>Att26</th>\n",
       "      <th>Att27</th>\n",
       "      <th>Att28</th>\n",
       "      <th>Att29</th>\n",
       "      <th>Att30</th>\n",
       "      <th>Att31</th>\n",
       "      <th>Att32</th>\n",
       "      <th>Att33</th>\n",
       "      <th>Att34</th>\n",
       "      <th>Att35</th>\n",
       "      <th>Att36</th>\n",
       "      <th>Att37</th>\n",
       "      <th>Att38</th>\n",
       "      <th>Att39</th>\n",
       "      <th>Att40</th>\n",
       "      <th>...</th>\n",
       "      <th>Att78</th>\n",
       "      <th>Att79</th>\n",
       "      <th>Att80</th>\n",
       "      <th>Att81</th>\n",
       "      <th>Att82</th>\n",
       "      <th>Att83</th>\n",
       "      <th>Att84</th>\n",
       "      <th>Att85</th>\n",
       "      <th>Att86</th>\n",
       "      <th>Att87</th>\n",
       "      <th>Att88</th>\n",
       "      <th>Att89</th>\n",
       "      <th>Att90</th>\n",
       "      <th>Att91</th>\n",
       "      <th>Att92</th>\n",
       "      <th>Att93</th>\n",
       "      <th>Att94</th>\n",
       "      <th>Att95</th>\n",
       "      <th>Att96</th>\n",
       "      <th>Att97</th>\n",
       "      <th>Att98</th>\n",
       "      <th>Att99</th>\n",
       "      <th>Att100</th>\n",
       "      <th>Att101</th>\n",
       "      <th>Att102</th>\n",
       "      <th>Att103</th>\n",
       "      <th>Class1</th>\n",
       "      <th>Class2</th>\n",
       "      <th>Class3</th>\n",
       "      <th>Class4</th>\n",
       "      <th>Class5</th>\n",
       "      <th>Class6</th>\n",
       "      <th>Class7</th>\n",
       "      <th>Class8</th>\n",
       "      <th>Class9</th>\n",
       "      <th>Class10</th>\n",
       "      <th>Class11</th>\n",
       "      <th>Class12</th>\n",
       "      <th>Class13</th>\n",
       "      <th>Class14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.004168</td>\n",
       "      <td>-0.170975</td>\n",
       "      <td>-0.156748</td>\n",
       "      <td>-0.142151</td>\n",
       "      <td>0.058781</td>\n",
       "      <td>0.026851</td>\n",
       "      <td>0.197719</td>\n",
       "      <td>0.041850</td>\n",
       "      <td>0.066938</td>\n",
       "      <td>-0.056617</td>\n",
       "      <td>-0.027230</td>\n",
       "      <td>-0.137411</td>\n",
       "      <td>0.067776</td>\n",
       "      <td>0.047175</td>\n",
       "      <td>0.155671</td>\n",
       "      <td>0.050766</td>\n",
       "      <td>0.102557</td>\n",
       "      <td>-0.020259</td>\n",
       "      <td>-0.200512</td>\n",
       "      <td>-0.095371</td>\n",
       "      <td>-0.081940</td>\n",
       "      <td>-0.103735</td>\n",
       "      <td>0.093299</td>\n",
       "      <td>0.105475</td>\n",
       "      <td>0.148560</td>\n",
       "      <td>0.085925</td>\n",
       "      <td>0.107879</td>\n",
       "      <td>0.108075</td>\n",
       "      <td>0.085388</td>\n",
       "      <td>0.124026</td>\n",
       "      <td>-0.003650</td>\n",
       "      <td>-0.127376</td>\n",
       "      <td>0.039394</td>\n",
       "      <td>-0.018364</td>\n",
       "      <td>0.050378</td>\n",
       "      <td>0.157190</td>\n",
       "      <td>0.203563</td>\n",
       "      <td>0.111552</td>\n",
       "      <td>0.017907</td>\n",
       "      <td>-0.001126</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.175325</td>\n",
       "      <td>-0.133636</td>\n",
       "      <td>0.005524</td>\n",
       "      <td>-0.014981</td>\n",
       "      <td>-0.031946</td>\n",
       "      <td>-0.015114</td>\n",
       "      <td>-0.047175</td>\n",
       "      <td>0.003829</td>\n",
       "      <td>0.010967</td>\n",
       "      <td>-0.006062</td>\n",
       "      <td>-0.027560</td>\n",
       "      <td>-0.019866</td>\n",
       "      <td>-0.024046</td>\n",
       "      <td>-0.025153</td>\n",
       "      <td>-0.009261</td>\n",
       "      <td>-0.025539</td>\n",
       "      <td>0.006166</td>\n",
       "      <td>-0.012976</td>\n",
       "      <td>-0.014259</td>\n",
       "      <td>-0.015024</td>\n",
       "      <td>-0.010747</td>\n",
       "      <td>0.000411</td>\n",
       "      <td>-0.032056</td>\n",
       "      <td>-0.018312</td>\n",
       "      <td>0.030126</td>\n",
       "      <td>0.124722</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.103956</td>\n",
       "      <td>0.011879</td>\n",
       "      <td>-0.098986</td>\n",
       "      <td>-0.054501</td>\n",
       "      <td>-0.007970</td>\n",
       "      <td>0.049113</td>\n",
       "      <td>-0.030580</td>\n",
       "      <td>-0.077933</td>\n",
       "      <td>-0.080529</td>\n",
       "      <td>-0.016267</td>\n",
       "      <td>-0.215304</td>\n",
       "      <td>-0.009885</td>\n",
       "      <td>-0.155843</td>\n",
       "      <td>-0.059522</td>\n",
       "      <td>-0.098836</td>\n",
       "      <td>-0.071141</td>\n",
       "      <td>-0.023494</td>\n",
       "      <td>-0.071200</td>\n",
       "      <td>0.027767</td>\n",
       "      <td>0.003091</td>\n",
       "      <td>-0.003761</td>\n",
       "      <td>0.074600</td>\n",
       "      <td>0.053080</td>\n",
       "      <td>-0.008138</td>\n",
       "      <td>0.001794</td>\n",
       "      <td>-0.111704</td>\n",
       "      <td>-0.140291</td>\n",
       "      <td>-0.063347</td>\n",
       "      <td>0.066767</td>\n",
       "      <td>-0.167073</td>\n",
       "      <td>-0.095567</td>\n",
       "      <td>-0.047209</td>\n",
       "      <td>0.082206</td>\n",
       "      <td>0.144445</td>\n",
       "      <td>0.086581</td>\n",
       "      <td>-0.111850</td>\n",
       "      <td>-0.086560</td>\n",
       "      <td>0.024942</td>\n",
       "      <td>-0.131539</td>\n",
       "      <td>0.080062</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001249</td>\n",
       "      <td>-0.020209</td>\n",
       "      <td>-0.077359</td>\n",
       "      <td>-0.045139</td>\n",
       "      <td>-0.074738</td>\n",
       "      <td>0.051846</td>\n",
       "      <td>0.009323</td>\n",
       "      <td>0.184332</td>\n",
       "      <td>0.420424</td>\n",
       "      <td>-0.090224</td>\n",
       "      <td>-0.090718</td>\n",
       "      <td>-0.035266</td>\n",
       "      <td>-0.046729</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>-0.066023</td>\n",
       "      <td>-0.051916</td>\n",
       "      <td>0.007680</td>\n",
       "      <td>0.027719</td>\n",
       "      <td>-0.085811</td>\n",
       "      <td>0.111123</td>\n",
       "      <td>0.050541</td>\n",
       "      <td>0.027565</td>\n",
       "      <td>-0.063569</td>\n",
       "      <td>-0.041471</td>\n",
       "      <td>-0.079758</td>\n",
       "      <td>0.017161</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.509949</td>\n",
       "      <td>0.401709</td>\n",
       "      <td>0.293799</td>\n",
       "      <td>0.087714</td>\n",
       "      <td>0.011686</td>\n",
       "      <td>-0.006411</td>\n",
       "      <td>-0.006255</td>\n",
       "      <td>0.013646</td>\n",
       "      <td>-0.040666</td>\n",
       "      <td>-0.024447</td>\n",
       "      <td>-0.040576</td>\n",
       "      <td>0.014326</td>\n",
       "      <td>-0.074968</td>\n",
       "      <td>0.141365</td>\n",
       "      <td>-0.015182</td>\n",
       "      <td>0.013691</td>\n",
       "      <td>0.006893</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>-0.020726</td>\n",
       "      <td>-0.044104</td>\n",
       "      <td>-0.052959</td>\n",
       "      <td>-0.085572</td>\n",
       "      <td>-0.061547</td>\n",
       "      <td>-0.029578</td>\n",
       "      <td>0.027700</td>\n",
       "      <td>-0.094310</td>\n",
       "      <td>-0.047721</td>\n",
       "      <td>-0.081589</td>\n",
       "      <td>-0.022846</td>\n",
       "      <td>-0.106684</td>\n",
       "      <td>-0.068873</td>\n",
       "      <td>-0.105225</td>\n",
       "      <td>-0.065414</td>\n",
       "      <td>-0.047722</td>\n",
       "      <td>-0.070723</td>\n",
       "      <td>-0.057425</td>\n",
       "      <td>-0.042024</td>\n",
       "      <td>-0.034122</td>\n",
       "      <td>-0.049606</td>\n",
       "      <td>0.015137</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002432</td>\n",
       "      <td>0.001711</td>\n",
       "      <td>-0.083572</td>\n",
       "      <td>-0.096943</td>\n",
       "      <td>0.148457</td>\n",
       "      <td>-0.007413</td>\n",
       "      <td>0.130691</td>\n",
       "      <td>-0.032325</td>\n",
       "      <td>0.028612</td>\n",
       "      <td>-0.023051</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.103336</td>\n",
       "      <td>0.138232</td>\n",
       "      <td>-0.100351</td>\n",
       "      <td>0.140423</td>\n",
       "      <td>0.110074</td>\n",
       "      <td>0.096277</td>\n",
       "      <td>-0.044932</td>\n",
       "      <td>-0.089470</td>\n",
       "      <td>-0.009162</td>\n",
       "      <td>-0.012010</td>\n",
       "      <td>0.308378</td>\n",
       "      <td>-0.028053</td>\n",
       "      <td>0.026710</td>\n",
       "      <td>-0.066565</td>\n",
       "      <td>-0.122352</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.119092</td>\n",
       "      <td>0.004412</td>\n",
       "      <td>-0.002262</td>\n",
       "      <td>0.072254</td>\n",
       "      <td>0.044512</td>\n",
       "      <td>-0.051467</td>\n",
       "      <td>0.074686</td>\n",
       "      <td>-0.007670</td>\n",
       "      <td>0.079438</td>\n",
       "      <td>0.062184</td>\n",
       "      <td>-0.013027</td>\n",
       "      <td>0.045538</td>\n",
       "      <td>0.080412</td>\n",
       "      <td>-0.010042</td>\n",
       "      <td>0.013029</td>\n",
       "      <td>-0.071975</td>\n",
       "      <td>0.089818</td>\n",
       "      <td>-0.016129</td>\n",
       "      <td>0.033105</td>\n",
       "      <td>0.024275</td>\n",
       "      <td>0.040428</td>\n",
       "      <td>0.064248</td>\n",
       "      <td>0.225613</td>\n",
       "      <td>0.176576</td>\n",
       "      <td>0.015501</td>\n",
       "      <td>0.009491</td>\n",
       "      <td>-0.013684</td>\n",
       "      <td>-0.017633</td>\n",
       "      <td>0.085007</td>\n",
       "      <td>-0.056274</td>\n",
       "      <td>-0.088925</td>\n",
       "      <td>-0.062951</td>\n",
       "      <td>0.227151</td>\n",
       "      <td>0.165897</td>\n",
       "      <td>0.150224</td>\n",
       "      <td>0.065105</td>\n",
       "      <td>0.110891</td>\n",
       "      <td>0.048451</td>\n",
       "      <td>0.114726</td>\n",
       "      <td>0.020393</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111806</td>\n",
       "      <td>-0.154732</td>\n",
       "      <td>0.302807</td>\n",
       "      <td>0.340027</td>\n",
       "      <td>-0.093332</td>\n",
       "      <td>-0.057848</td>\n",
       "      <td>-0.010558</td>\n",
       "      <td>-0.039194</td>\n",
       "      <td>-0.041628</td>\n",
       "      <td>-0.077455</td>\n",
       "      <td>-0.008553</td>\n",
       "      <td>-0.022404</td>\n",
       "      <td>-0.106131</td>\n",
       "      <td>-0.103067</td>\n",
       "      <td>-0.083059</td>\n",
       "      <td>-0.089064</td>\n",
       "      <td>-0.083809</td>\n",
       "      <td>0.200354</td>\n",
       "      <td>-0.075716</td>\n",
       "      <td>0.196605</td>\n",
       "      <td>0.152758</td>\n",
       "      <td>-0.028484</td>\n",
       "      <td>-0.074207</td>\n",
       "      <td>-0.089227</td>\n",
       "      <td>-0.049913</td>\n",
       "      <td>-0.043893</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.042037</td>\n",
       "      <td>0.007054</td>\n",
       "      <td>-0.069483</td>\n",
       "      <td>0.081015</td>\n",
       "      <td>-0.048207</td>\n",
       "      <td>0.089446</td>\n",
       "      <td>-0.004947</td>\n",
       "      <td>0.064456</td>\n",
       "      <td>-0.133387</td>\n",
       "      <td>0.068878</td>\n",
       "      <td>-0.139371</td>\n",
       "      <td>0.041487</td>\n",
       "      <td>-0.058531</td>\n",
       "      <td>0.021264</td>\n",
       "      <td>-0.101382</td>\n",
       "      <td>0.021015</td>\n",
       "      <td>0.096572</td>\n",
       "      <td>-0.005136</td>\n",
       "      <td>0.111104</td>\n",
       "      <td>-0.008323</td>\n",
       "      <td>0.020210</td>\n",
       "      <td>-0.003967</td>\n",
       "      <td>0.039762</td>\n",
       "      <td>0.006744</td>\n",
       "      <td>-0.041730</td>\n",
       "      <td>-0.174533</td>\n",
       "      <td>-0.101343</td>\n",
       "      <td>-0.115674</td>\n",
       "      <td>0.328511</td>\n",
       "      <td>-0.108945</td>\n",
       "      <td>-0.160748</td>\n",
       "      <td>-0.120290</td>\n",
       "      <td>-0.148308</td>\n",
       "      <td>-0.082882</td>\n",
       "      <td>-0.127218</td>\n",
       "      <td>-0.167186</td>\n",
       "      <td>-0.143210</td>\n",
       "      <td>-0.118028</td>\n",
       "      <td>-0.297516</td>\n",
       "      <td>-0.160082</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108388</td>\n",
       "      <td>0.095516</td>\n",
       "      <td>0.015942</td>\n",
       "      <td>0.087354</td>\n",
       "      <td>0.176911</td>\n",
       "      <td>-0.062311</td>\n",
       "      <td>0.117205</td>\n",
       "      <td>-0.048277</td>\n",
       "      <td>-0.053679</td>\n",
       "      <td>0.014850</td>\n",
       "      <td>-0.066453</td>\n",
       "      <td>-0.067962</td>\n",
       "      <td>-0.083653</td>\n",
       "      <td>-0.081130</td>\n",
       "      <td>-0.061469</td>\n",
       "      <td>0.023662</td>\n",
       "      <td>-0.060467</td>\n",
       "      <td>0.044351</td>\n",
       "      <td>-0.057209</td>\n",
       "      <td>0.028047</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>-0.050026</td>\n",
       "      <td>0.023248</td>\n",
       "      <td>-0.061539</td>\n",
       "      <td>-0.035160</td>\n",
       "      <td>0.067834</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2412</th>\n",
       "      <td>-0.119784</td>\n",
       "      <td>0.001259</td>\n",
       "      <td>-0.123645</td>\n",
       "      <td>-0.015513</td>\n",
       "      <td>-0.059683</td>\n",
       "      <td>0.091032</td>\n",
       "      <td>-0.043302</td>\n",
       "      <td>0.229219</td>\n",
       "      <td>-0.071498</td>\n",
       "      <td>0.182709</td>\n",
       "      <td>-0.169902</td>\n",
       "      <td>0.254843</td>\n",
       "      <td>-0.179968</td>\n",
       "      <td>0.173563</td>\n",
       "      <td>-0.060754</td>\n",
       "      <td>0.111926</td>\n",
       "      <td>0.055960</td>\n",
       "      <td>0.293560</td>\n",
       "      <td>0.017478</td>\n",
       "      <td>-0.081646</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>-0.079165</td>\n",
       "      <td>-0.030758</td>\n",
       "      <td>0.057605</td>\n",
       "      <td>-0.069617</td>\n",
       "      <td>-0.152909</td>\n",
       "      <td>-0.013818</td>\n",
       "      <td>-0.134586</td>\n",
       "      <td>0.035602</td>\n",
       "      <td>-0.111456</td>\n",
       "      <td>-0.013309</td>\n",
       "      <td>-0.169705</td>\n",
       "      <td>-0.116210</td>\n",
       "      <td>-0.088351</td>\n",
       "      <td>-0.059824</td>\n",
       "      <td>0.055180</td>\n",
       "      <td>-0.015347</td>\n",
       "      <td>-0.054320</td>\n",
       "      <td>-0.158766</td>\n",
       "      <td>-0.038536</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057122</td>\n",
       "      <td>0.103497</td>\n",
       "      <td>-0.064997</td>\n",
       "      <td>-0.019628</td>\n",
       "      <td>0.012507</td>\n",
       "      <td>0.209633</td>\n",
       "      <td>-0.081401</td>\n",
       "      <td>-0.057052</td>\n",
       "      <td>-0.077347</td>\n",
       "      <td>-0.076410</td>\n",
       "      <td>-0.077983</td>\n",
       "      <td>-0.021983</td>\n",
       "      <td>0.053034</td>\n",
       "      <td>0.191556</td>\n",
       "      <td>0.183931</td>\n",
       "      <td>0.065281</td>\n",
       "      <td>0.024084</td>\n",
       "      <td>-0.055915</td>\n",
       "      <td>-0.055593</td>\n",
       "      <td>-0.049642</td>\n",
       "      <td>0.018571</td>\n",
       "      <td>0.068742</td>\n",
       "      <td>-0.061001</td>\n",
       "      <td>-0.081132</td>\n",
       "      <td>-0.065844</td>\n",
       "      <td>0.001267</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2413</th>\n",
       "      <td>0.085327</td>\n",
       "      <td>0.058590</td>\n",
       "      <td>0.085268</td>\n",
       "      <td>-0.020897</td>\n",
       "      <td>0.068972</td>\n",
       "      <td>0.030125</td>\n",
       "      <td>0.078056</td>\n",
       "      <td>0.011346</td>\n",
       "      <td>0.052618</td>\n",
       "      <td>0.066093</td>\n",
       "      <td>0.028501</td>\n",
       "      <td>0.037778</td>\n",
       "      <td>0.056401</td>\n",
       "      <td>0.073084</td>\n",
       "      <td>0.054199</td>\n",
       "      <td>0.010155</td>\n",
       "      <td>0.033694</td>\n",
       "      <td>0.022873</td>\n",
       "      <td>0.075112</td>\n",
       "      <td>0.092810</td>\n",
       "      <td>0.098595</td>\n",
       "      <td>0.059712</td>\n",
       "      <td>0.089367</td>\n",
       "      <td>0.056306</td>\n",
       "      <td>-0.010929</td>\n",
       "      <td>0.029214</td>\n",
       "      <td>0.028160</td>\n",
       "      <td>0.017634</td>\n",
       "      <td>0.005513</td>\n",
       "      <td>0.047156</td>\n",
       "      <td>0.100123</td>\n",
       "      <td>0.102521</td>\n",
       "      <td>-0.038055</td>\n",
       "      <td>-0.092468</td>\n",
       "      <td>-0.096875</td>\n",
       "      <td>-0.026086</td>\n",
       "      <td>0.048201</td>\n",
       "      <td>0.062117</td>\n",
       "      <td>0.052804</td>\n",
       "      <td>0.005024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030045</td>\n",
       "      <td>-0.049208</td>\n",
       "      <td>-0.061023</td>\n",
       "      <td>-0.073127</td>\n",
       "      <td>-0.054131</td>\n",
       "      <td>0.230720</td>\n",
       "      <td>-0.054853</td>\n",
       "      <td>0.137628</td>\n",
       "      <td>0.150380</td>\n",
       "      <td>-0.029207</td>\n",
       "      <td>0.198999</td>\n",
       "      <td>0.240646</td>\n",
       "      <td>-0.102721</td>\n",
       "      <td>-0.099789</td>\n",
       "      <td>-0.078345</td>\n",
       "      <td>-0.084716</td>\n",
       "      <td>-0.079992</td>\n",
       "      <td>-0.075444</td>\n",
       "      <td>0.294987</td>\n",
       "      <td>-0.076379</td>\n",
       "      <td>-0.076293</td>\n",
       "      <td>-0.072451</td>\n",
       "      <td>-0.052258</td>\n",
       "      <td>-0.040026</td>\n",
       "      <td>0.342176</td>\n",
       "      <td>-0.169668</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2414</th>\n",
       "      <td>0.082526</td>\n",
       "      <td>-0.095571</td>\n",
       "      <td>-0.022019</td>\n",
       "      <td>-0.046793</td>\n",
       "      <td>-0.038360</td>\n",
       "      <td>0.041084</td>\n",
       "      <td>0.056509</td>\n",
       "      <td>0.011749</td>\n",
       "      <td>-0.029657</td>\n",
       "      <td>-0.012198</td>\n",
       "      <td>-0.008540</td>\n",
       "      <td>-0.013902</td>\n",
       "      <td>-0.068013</td>\n",
       "      <td>0.042327</td>\n",
       "      <td>0.052293</td>\n",
       "      <td>0.050993</td>\n",
       "      <td>0.082892</td>\n",
       "      <td>-0.009390</td>\n",
       "      <td>0.029446</td>\n",
       "      <td>0.053463</td>\n",
       "      <td>0.074409</td>\n",
       "      <td>0.091427</td>\n",
       "      <td>0.059711</td>\n",
       "      <td>-0.020000</td>\n",
       "      <td>-0.020922</td>\n",
       "      <td>-0.073768</td>\n",
       "      <td>-0.061061</td>\n",
       "      <td>-0.091906</td>\n",
       "      <td>-0.046665</td>\n",
       "      <td>-0.041211</td>\n",
       "      <td>-0.047039</td>\n",
       "      <td>0.040556</td>\n",
       "      <td>-0.071408</td>\n",
       "      <td>-0.162032</td>\n",
       "      <td>-0.163358</td>\n",
       "      <td>0.012145</td>\n",
       "      <td>0.076758</td>\n",
       "      <td>-0.005729</td>\n",
       "      <td>-0.026470</td>\n",
       "      <td>-0.108322</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.083398</td>\n",
       "      <td>-0.059522</td>\n",
       "      <td>-0.004905</td>\n",
       "      <td>-0.069757</td>\n",
       "      <td>0.293519</td>\n",
       "      <td>0.164906</td>\n",
       "      <td>0.172683</td>\n",
       "      <td>-0.024597</td>\n",
       "      <td>-0.056481</td>\n",
       "      <td>0.086025</td>\n",
       "      <td>-0.070759</td>\n",
       "      <td>-0.076122</td>\n",
       "      <td>-0.076408</td>\n",
       "      <td>-0.064713</td>\n",
       "      <td>-0.040290</td>\n",
       "      <td>-0.077142</td>\n",
       "      <td>-0.006624</td>\n",
       "      <td>-0.036850</td>\n",
       "      <td>-0.064831</td>\n",
       "      <td>-0.068696</td>\n",
       "      <td>-0.068521</td>\n",
       "      <td>-0.039841</td>\n",
       "      <td>0.274575</td>\n",
       "      <td>-0.066957</td>\n",
       "      <td>0.260121</td>\n",
       "      <td>-0.125303</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2415</th>\n",
       "      <td>-0.130830</td>\n",
       "      <td>0.008868</td>\n",
       "      <td>-0.009457</td>\n",
       "      <td>-0.058930</td>\n",
       "      <td>-0.041224</td>\n",
       "      <td>0.042269</td>\n",
       "      <td>0.117717</td>\n",
       "      <td>0.037388</td>\n",
       "      <td>-0.085563</td>\n",
       "      <td>0.136649</td>\n",
       "      <td>-0.255284</td>\n",
       "      <td>-0.334406</td>\n",
       "      <td>-0.194436</td>\n",
       "      <td>-0.046137</td>\n",
       "      <td>0.049138</td>\n",
       "      <td>0.014249</td>\n",
       "      <td>0.063691</td>\n",
       "      <td>-0.065423</td>\n",
       "      <td>-0.084182</td>\n",
       "      <td>0.013208</td>\n",
       "      <td>-0.043259</td>\n",
       "      <td>-0.122727</td>\n",
       "      <td>-0.119400</td>\n",
       "      <td>-0.082374</td>\n",
       "      <td>-0.033362</td>\n",
       "      <td>-0.012996</td>\n",
       "      <td>0.014870</td>\n",
       "      <td>-0.023433</td>\n",
       "      <td>-0.023071</td>\n",
       "      <td>0.020958</td>\n",
       "      <td>0.035999</td>\n",
       "      <td>-0.025947</td>\n",
       "      <td>-0.079952</td>\n",
       "      <td>-0.137046</td>\n",
       "      <td>0.058524</td>\n",
       "      <td>0.093744</td>\n",
       "      <td>0.109146</td>\n",
       "      <td>0.013801</td>\n",
       "      <td>-0.008219</td>\n",
       "      <td>0.063381</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023212</td>\n",
       "      <td>0.000583</td>\n",
       "      <td>0.038050</td>\n",
       "      <td>-0.113469</td>\n",
       "      <td>-0.130450</td>\n",
       "      <td>-0.109596</td>\n",
       "      <td>0.032658</td>\n",
       "      <td>0.147271</td>\n",
       "      <td>0.108450</td>\n",
       "      <td>0.023597</td>\n",
       "      <td>0.195494</td>\n",
       "      <td>-0.093824</td>\n",
       "      <td>-0.072476</td>\n",
       "      <td>-0.034463</td>\n",
       "      <td>-0.000725</td>\n",
       "      <td>-0.114834</td>\n",
       "      <td>0.085087</td>\n",
       "      <td>0.033166</td>\n",
       "      <td>-0.012710</td>\n",
       "      <td>0.135359</td>\n",
       "      <td>0.213512</td>\n",
       "      <td>-0.107561</td>\n",
       "      <td>-0.081925</td>\n",
       "      <td>-0.122332</td>\n",
       "      <td>-0.022453</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2416</th>\n",
       "      <td>-0.171578</td>\n",
       "      <td>-0.066536</td>\n",
       "      <td>0.168206</td>\n",
       "      <td>0.246831</td>\n",
       "      <td>0.079555</td>\n",
       "      <td>0.016528</td>\n",
       "      <td>-0.088908</td>\n",
       "      <td>-0.212926</td>\n",
       "      <td>-0.280230</td>\n",
       "      <td>-0.187064</td>\n",
       "      <td>0.021891</td>\n",
       "      <td>0.257780</td>\n",
       "      <td>0.181716</td>\n",
       "      <td>0.122069</td>\n",
       "      <td>0.008877</td>\n",
       "      <td>-0.067158</td>\n",
       "      <td>-0.067343</td>\n",
       "      <td>-0.168937</td>\n",
       "      <td>-0.124432</td>\n",
       "      <td>-0.172594</td>\n",
       "      <td>0.053653</td>\n",
       "      <td>0.009450</td>\n",
       "      <td>0.053303</td>\n",
       "      <td>0.090438</td>\n",
       "      <td>0.085904</td>\n",
       "      <td>0.069243</td>\n",
       "      <td>0.030781</td>\n",
       "      <td>0.074417</td>\n",
       "      <td>0.066347</td>\n",
       "      <td>-0.050537</td>\n",
       "      <td>-0.046217</td>\n",
       "      <td>0.106744</td>\n",
       "      <td>-0.081458</td>\n",
       "      <td>0.173149</td>\n",
       "      <td>0.196126</td>\n",
       "      <td>0.057334</td>\n",
       "      <td>-0.046731</td>\n",
       "      <td>-0.171364</td>\n",
       "      <td>0.086061</td>\n",
       "      <td>0.148047</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037724</td>\n",
       "      <td>-0.097965</td>\n",
       "      <td>-0.062793</td>\n",
       "      <td>0.020469</td>\n",
       "      <td>-0.091561</td>\n",
       "      <td>-0.056605</td>\n",
       "      <td>0.087247</td>\n",
       "      <td>-0.012466</td>\n",
       "      <td>-0.037839</td>\n",
       "      <td>-0.074064</td>\n",
       "      <td>-0.076054</td>\n",
       "      <td>0.081239</td>\n",
       "      <td>-0.007374</td>\n",
       "      <td>0.056464</td>\n",
       "      <td>0.054823</td>\n",
       "      <td>0.034766</td>\n",
       "      <td>0.360199</td>\n",
       "      <td>-0.058178</td>\n",
       "      <td>-0.003104</td>\n",
       "      <td>-0.016028</td>\n",
       "      <td>0.054244</td>\n",
       "      <td>-0.017797</td>\n",
       "      <td>-0.081870</td>\n",
       "      <td>-0.083342</td>\n",
       "      <td>-0.063135</td>\n",
       "      <td>0.018810</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2417 rows Ã— 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Att1      Att2      Att3  ...  Class12  Class13  Class14\n",
       "0     0.004168 -0.170975 -0.156748  ...        1        1        0\n",
       "1    -0.103956  0.011879 -0.098986  ...        0        0        0\n",
       "2     0.509949  0.401709  0.293799  ...        1        1        0\n",
       "3     0.119092  0.004412 -0.002262  ...        0        0        0\n",
       "4     0.042037  0.007054 -0.069483  ...        0        0        0\n",
       "...        ...       ...       ...  ...      ...      ...      ...\n",
       "2412 -0.119784  0.001259 -0.123645  ...        0        0        0\n",
       "2413  0.085327  0.058590  0.085268  ...        1        1        0\n",
       "2414  0.082526 -0.095571 -0.022019  ...        1        1        0\n",
       "2415 -0.130830  0.008868 -0.009457  ...        1        1        0\n",
       "2416 -0.171578 -0.066536  0.168206  ...        1        1        0\n",
       "\n",
       "[2417 rows x 117 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('https://raw.githubusercontent.com/Susmitha1694/AdvancedMachineLearning/master/yeast.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uqn_CS3t1ojr"
   },
   "source": [
    "## Task 1: Implement the Binary Relevance Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6FvsLLRN1ojs"
   },
   "outputs": [],
   "source": [
    "class BinaryRelevanceAlg(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    # Constructor for the classifier object\n",
    "    def __init__(self, PredictClassifier=GaussianNB()):\n",
    "        self.PredictClassifier = PredictClassifier\n",
    "\n",
    "    # The fit function to train a classifier\n",
    "    def fit(self, X, y):\n",
    "    \n",
    "        models = []\n",
    "        \n",
    "        #For each label in the data set training a model of the given classifier on all the features \n",
    "        #and storing all individual models in a list.\n",
    "        for instance in y.columns:\n",
    "            X, y[instance] = check_X_y(X, y[instance])\n",
    "            model = copy.copy(self.PredictClassifier)\n",
    "            model.fit(X,y[instance])\n",
    "            models.append(model)\n",
    "        \n",
    "        self.models_ = models\n",
    "        \n",
    "        # Return the classifier\n",
    "        return self\n",
    "\n",
    "    # The predict function to make a set of predictions for a set of query instances\n",
    "    def predict(self, X):\n",
    "\n",
    "        # Check that the input features match the type and shape of the training features\n",
    "        X = check_array(X)\n",
    "        \n",
    "        # Initialise an empty list to store the predictions made\n",
    "        predictions = list()\n",
    "        \n",
    "        #loop thorugh all models in the list to make the predictions for each label\n",
    "        for model in self.models_:\n",
    "            predict = model.predict(X)\n",
    "            predictions.append(predict)\n",
    "        return np.array(predictions).T\n",
    "    \n",
    "    \n",
    "    # The predict function to make a set of predictions for a set of query instances\n",
    "    def predict_proba(self, X):\n",
    "\n",
    "        # Check that the input features match the type and shape of the training features\n",
    "        X = check_array(X)\n",
    "        \n",
    "         # Initialise an empty list to store the predictions made\n",
    "        predict_proba_list = list()\n",
    "        \n",
    "        #loop thorugh all models in the list to make the predictions and probabilites for each label\n",
    "        for model in self.models_:\n",
    "            predict_pro = model.predict_proba(X)\n",
    "            predict_proba_list.append(predict_pro)\n",
    "            \n",
    "        return np.array(predict_proba_list).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m5qNV_L81ojv"
   },
   "outputs": [],
   "source": [
    "#Splitting the data set in to features and labels\n",
    "x = dataset.iloc[:,:-14]\n",
    "y = dataset.iloc[:,-14:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cDGRWGUi1ojy"
   },
   "outputs": [],
   "source": [
    "#Dividing the features and labels into training set and test set\n",
    "X_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "10GxBh511oj2"
   },
   "outputs": [],
   "source": [
    "#Lists to store the hamming loss values for each algorithm\n",
    "binaryRelevanceList = []\n",
    "binaryRelevanceWithUndersamplingList = []\n",
    "chainClassificationList = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DvZdsf3n1oj5"
   },
   "source": [
    "## Evaluation of Binary Relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hv4Jjaa51oj6"
   },
   "source": [
    "### Classifier 1 : Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "gwWE0cJg1oj7",
    "outputId": "213686c9-16f5-4004-ad39-88b462906d4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinaryRelevanceAlg(PredictClassifier=GaussianNB(priors=None,\n",
       "                                                var_smoothing=1e-09))"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_NB = BinaryRelevanceAlg(GaussianNB())\n",
    "my_model_NB.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "colab_type": "code",
    "id": "dlhzM3-F1oj_",
    "outputId": "23fe59e8-ca47-4f89-8fca-14d3b4051c30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hamming_loss: 0.29412632821723733\n",
      "\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.65      0.60       143\n",
      "           1       0.52      0.67      0.59       206\n",
      "           2       0.65      0.76      0.70       201\n",
      "           3       0.65      0.58      0.61       180\n",
      "           4       0.57      0.51      0.54       152\n",
      "           5       0.41      0.53      0.46       124\n",
      "           6       0.29      0.49      0.37        90\n",
      "           7       0.31      0.54      0.39       102\n",
      "           8       0.08      0.23      0.12        40\n",
      "           9       0.22      0.48      0.30        56\n",
      "          10       0.24      0.39      0.30        70\n",
      "          11       0.82      0.65      0.73       369\n",
      "          12       0.82      0.65      0.72       364\n",
      "          13       0.04      0.22      0.07         9\n",
      "\n",
      "   micro avg       0.52      0.60      0.56      2106\n",
      "   macro avg       0.44      0.52      0.46      2106\n",
      "weighted avg       0.60      0.60      0.59      2106\n",
      " samples avg       0.55      0.61      0.55      2106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make a set of predictions for the training data\n",
    "y_pred = my_model_NB.predict(x_test)\n",
    "\n",
    "ham_loss = hamming_loss(y_test, y_pred)\n",
    "print(\"hamming_loss: \" +  str(ham_loss))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bkn1u7iN1okC"
   },
   "outputs": [],
   "source": [
    "binaryRelevanceList.append(round(ham_loss,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "colab_type": "code",
    "id": "-Neug7pH1okE",
    "outputId": "550cc66f-7177-4a48-8144-350d5346ee9e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[9.96639774e-01, 9.66466695e-01, 5.59303472e-01, ...,\n",
       "         9.34292456e-03, 7.36342450e-03, 9.99845643e-01],\n",
       "        [1.07392885e-01, 2.02884152e-04, 2.23900907e-02, ...,\n",
       "         9.51656415e-01, 9.37571401e-01, 9.99960328e-01],\n",
       "        [9.87501495e-01, 8.00867502e-01, 1.47116645e-01, ...,\n",
       "         7.04888243e-03, 8.99262034e-03, 9.99994629e-01],\n",
       "        ...,\n",
       "        [2.12813779e-07, 5.40235847e-03, 9.99924442e-01, ...,\n",
       "         9.99448474e-01, 9.99220187e-01, 1.00000000e+00],\n",
       "        [1.12346569e-01, 5.98445103e-02, 6.92549532e-01, ...,\n",
       "         2.08065998e-01, 2.44976099e-01, 9.99999011e-01],\n",
       "        [9.92305132e-01, 2.83575749e-03, 3.84515585e-05, ...,\n",
       "         6.37845352e-01, 5.29838130e-01, 6.84220876e-04]],\n",
       "\n",
       "       [[3.36022634e-03, 3.35333046e-02, 4.40696528e-01, ...,\n",
       "         9.90657075e-01, 9.92636575e-01, 1.54356765e-04],\n",
       "        [8.92607115e-01, 9.99797116e-01, 9.77609909e-01, ...,\n",
       "         4.83435849e-02, 6.24285991e-02, 3.96722835e-05],\n",
       "        [1.24985049e-02, 1.99132498e-01, 8.52883355e-01, ...,\n",
       "         9.92951118e-01, 9.91007380e-01, 5.37138409e-06],\n",
       "        ...,\n",
       "        [9.99999787e-01, 9.94597642e-01, 7.55583911e-05, ...,\n",
       "         5.51526393e-04, 7.79812699e-04, 1.00329320e-12],\n",
       "        [8.87653431e-01, 9.40155490e-01, 3.07450468e-01, ...,\n",
       "         7.91934002e-01, 7.55023901e-01, 9.89141799e-07],\n",
       "        [7.69486818e-03, 9.97164243e-01, 9.99961548e-01, ...,\n",
       "         3.62154648e-01, 4.70161870e-01, 9.99315779e-01]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob = my_model_NB.predict_proba(x_test)\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OqKN58Sx1okI"
   },
   "source": [
    "### Classifier 2 : Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "colab_type": "code",
    "id": "0Pjoq1T61okJ",
    "outputId": "7b0c2ed8-0385-482d-c769-8b73bc59db65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinaryRelevanceAlg(PredictClassifier=DecisionTreeClassifier(ccp_alpha=0.0,\n",
       "                                                            class_weight=None,\n",
       "                                                            criterion='entropy',\n",
       "                                                            max_depth=None,\n",
       "                                                            max_features=None,\n",
       "                                                            max_leaf_nodes=None,\n",
       "                                                            min_impurity_decrease=0.0,\n",
       "                                                            min_impurity_split=None,\n",
       "                                                            min_samples_leaf=1,\n",
       "                                                            min_samples_split=2,\n",
       "                                                            min_weight_fraction_leaf=0.0,\n",
       "                                                            presort='deprecated',\n",
       "                                                            random_state=None,\n",
       "                                                            splitter='best'))"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_DT = BinaryRelevanceAlg(tree.DecisionTreeClassifier(criterion=\"entropy\"))\n",
    "my_model_DT.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "colab_type": "code",
    "id": "tpcALH5p1okM",
    "outputId": "196bd561-3270-4cbe-9eac-10cb8d4f5cc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hamming_loss: 0.27361275088547815\n",
      "\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.51      0.51       143\n",
      "           1       0.52      0.53      0.52       206\n",
      "           2       0.62      0.61      0.61       201\n",
      "           3       0.63      0.58      0.60       180\n",
      "           4       0.50      0.49      0.50       152\n",
      "           5       0.32      0.34      0.33       124\n",
      "           6       0.33      0.38      0.35        90\n",
      "           7       0.31      0.39      0.35       102\n",
      "           8       0.12      0.07      0.09        40\n",
      "           9       0.17      0.14      0.15        56\n",
      "          10       0.25      0.20      0.22        70\n",
      "          11       0.77      0.75      0.76       369\n",
      "          12       0.75      0.71      0.73       364\n",
      "          13       0.00      0.00      0.00         9\n",
      "\n",
      "   micro avg       0.56      0.55      0.56      2106\n",
      "   macro avg       0.41      0.41      0.41      2106\n",
      "weighted avg       0.56      0.55      0.56      2106\n",
      " samples avg       0.58      0.56      0.54      2106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make a set of predictions for the training data\n",
    "y_pred = my_model_DT.predict(x_test)\n",
    "\n",
    "ham_loss = hamming_loss(y_test, y_pred)\n",
    "print(\"hamming_loss: \" +  str(ham_loss))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p0wdNn471okQ"
   },
   "outputs": [],
   "source": [
    "binaryRelevanceList.append(round(ham_loss,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "colab_type": "code",
    "id": "Xya-bEHO1okS",
    "outputId": "7074e565-5a9e-4068-ee44-3bdd9bcc14c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 1., 0., ..., 1., 0., 1.],\n",
       "        [1., 0., 0., ..., 0., 0., 1.],\n",
       "        [1., 1., 1., ..., 0., 0., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1., ..., 0., 1., 1.],\n",
       "        [1., 1., 1., ..., 0., 0., 1.],\n",
       "        [1., 1., 0., ..., 1., 0., 1.]],\n",
       "\n",
       "       [[0., 0., 1., ..., 0., 1., 0.],\n",
       "        [0., 1., 1., ..., 1., 1., 0.],\n",
       "        [0., 0., 0., ..., 1., 1., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 1., 0., 0.],\n",
       "        [0., 0., 0., ..., 1., 1., 0.],\n",
       "        [0., 0., 1., ..., 0., 1., 0.]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob = my_model_DT.predict_proba(x_test)\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QIkJpMlx1okV"
   },
   "source": [
    "### Classifier 3 : Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "id": "LAb7MGRs1okW",
    "outputId": "18426b12-fd6d-42f5-e0ab-e94fdc288330"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinaryRelevanceAlg(PredictClassifier=RandomForestClassifier(bootstrap=True,\n",
       "                                                            ccp_alpha=0.0,\n",
       "                                                            class_weight=None,\n",
       "                                                            criterion='gini',\n",
       "                                                            max_depth=None,\n",
       "                                                            max_features=3,\n",
       "                                                            max_leaf_nodes=None,\n",
       "                                                            max_samples=None,\n",
       "                                                            min_impurity_decrease=0.0,\n",
       "                                                            min_impurity_split=None,\n",
       "                                                            min_samples_leaf=1,\n",
       "                                                            min_samples_split=200,\n",
       "                                                            min_weight_fraction_leaf=0.0,\n",
       "                                                            n_estimators=300,\n",
       "                                                            n_jobs=None,\n",
       "                                                            oob_score=False,\n",
       "                                                            random_state=None,\n",
       "                                                            verbose=0,\n",
       "                                                            warm_start=False))"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_RF = BinaryRelevanceAlg(ensemble.RandomForestClassifier(n_estimators=300, max_features = 3, min_samples_split=200))\n",
    "my_model_RF.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "colab_type": "code",
    "id": "_bGZxi6A1okY",
    "outputId": "cdaefc96-417c-4124-9030-de3d2fcf5b67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hamming_loss: 0.21384297520661158\n",
      "\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.16      0.27       143\n",
      "           1       0.65      0.28      0.39       206\n",
      "           2       0.76      0.34      0.47       201\n",
      "           3       0.94      0.25      0.39       180\n",
      "           4       0.96      0.16      0.27       152\n",
      "           5       0.00      0.00      0.00       124\n",
      "           6       0.00      0.00      0.00        90\n",
      "           7       0.00      0.00      0.00       102\n",
      "           8       0.00      0.00      0.00        40\n",
      "           9       0.00      0.00      0.00        56\n",
      "          10       0.00      0.00      0.00        70\n",
      "          11       0.76      1.00      0.87       369\n",
      "          12       0.75      1.00      0.86       364\n",
      "          13       0.00      0.00      0.00         9\n",
      "\n",
      "   micro avg       0.76      0.45      0.57      2106\n",
      "   macro avg       0.41      0.23      0.25      2106\n",
      "weighted avg       0.61      0.45      0.45      2106\n",
      " samples avg       0.76      0.45      0.54      2106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make a set of predictions for the training data\n",
    "y_pred = my_model_RF.predict(x_test)\n",
    "\n",
    "ham_loss = hamming_loss(y_test, y_pred)\n",
    "print(\"hamming_loss: \" +  str(ham_loss))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jWVq-Y_e1okb"
   },
   "outputs": [],
   "source": [
    "binaryRelevanceList.append(round(ham_loss,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "colab_type": "code",
    "id": "SXDcj9gp1okf",
    "outputId": "1bce685a-cd10-4d7d-9890-ae2bf365250b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.71201677, 0.61397731, 0.57326547, ..., 0.19742182,\n",
       "         0.21054889, 0.97982952],\n",
       "        [0.72440081, 0.49218882, 0.50877234, ..., 0.28283774,\n",
       "         0.26591634, 0.98885663],\n",
       "        [0.69451312, 0.53681155, 0.58080623, ..., 0.21215868,\n",
       "         0.21764025, 0.98884878],\n",
       "        ...,\n",
       "        [0.56064918, 0.50842828, 0.66919038, ..., 0.32100905,\n",
       "         0.32616277, 0.9935432 ],\n",
       "        [0.66089332, 0.54904818, 0.59566025, ..., 0.26470743,\n",
       "         0.29054009, 0.98366352],\n",
       "        [0.73726745, 0.52883667, 0.44884541, ..., 0.27595624,\n",
       "         0.28598776, 0.99058563]],\n",
       "\n",
       "       [[0.28798323, 0.38602269, 0.42673453, ..., 0.80257818,\n",
       "         0.78945111, 0.02017048],\n",
       "        [0.27559919, 0.50781118, 0.49122766, ..., 0.71716226,\n",
       "         0.73408366, 0.01114337],\n",
       "        [0.30548688, 0.46318845, 0.41919377, ..., 0.78784132,\n",
       "         0.78235975, 0.01115122],\n",
       "        ...,\n",
       "        [0.43935082, 0.49157172, 0.33080962, ..., 0.67899095,\n",
       "         0.67383723, 0.0064568 ],\n",
       "        [0.33910668, 0.45095182, 0.40433975, ..., 0.73529257,\n",
       "         0.70945991, 0.01633648],\n",
       "        [0.26273255, 0.47116333, 0.55115459, ..., 0.72404376,\n",
       "         0.71401224, 0.00941437]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob = my_model_RF.predict_proba(x_test)\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gLAwAUzF1okh"
   },
   "source": [
    "### Classifier 4 : Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "colab_type": "code",
    "id": "nM8xZRqO1oki",
    "outputId": "5eac25c4-9f07-4825-d0df-88d6e57670c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinaryRelevanceAlg(PredictClassifier=LogisticRegression(C=1.0,\n",
       "                                                        class_weight=None,\n",
       "                                                        dual=False,\n",
       "                                                        fit_intercept=True,\n",
       "                                                        intercept_scaling=1,\n",
       "                                                        l1_ratio=None,\n",
       "                                                        max_iter=100,\n",
       "                                                        multi_class='auto',\n",
       "                                                        n_jobs=None,\n",
       "                                                        penalty='l2',\n",
       "                                                        random_state=None,\n",
       "                                                        solver='lbfgs',\n",
       "                                                        tol=0.0001, verbose=0,\n",
       "                                                        warm_start=False))"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_LR = BinaryRelevanceAlg(linear_model.LogisticRegression())\n",
    "my_model_LR.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "colab_type": "code",
    "id": "2IGK00LZ1okl",
    "outputId": "08d3b576-b363-4cdc-d116-b48b6b5732e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hamming_loss: 0.20306965761511217\n",
      "\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.48      0.56       143\n",
      "           1       0.55      0.49      0.51       206\n",
      "           2       0.70      0.63      0.66       201\n",
      "           3       0.71      0.54      0.61       180\n",
      "           4       0.80      0.38      0.51       152\n",
      "           5       0.70      0.17      0.27       124\n",
      "           6       1.00      0.03      0.06        90\n",
      "           7       0.67      0.02      0.04       102\n",
      "           8       0.00      0.00      0.00        40\n",
      "           9       0.00      0.00      0.00        56\n",
      "          10       0.00      0.00      0.00        70\n",
      "          11       0.76      0.98      0.86       369\n",
      "          12       0.75      0.98      0.85       364\n",
      "          13       0.00      0.00      0.00         9\n",
      "\n",
      "   micro avg       0.72      0.57      0.63      2106\n",
      "   macro avg       0.52      0.34      0.35      2106\n",
      "weighted avg       0.66      0.57      0.56      2106\n",
      " samples avg       0.72      0.57      0.61      2106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make a set of predictions for the training data\n",
    "y_pred = my_model_LR.predict(x_test)\n",
    "\n",
    "ham_loss = hamming_loss(y_test, y_pred)\n",
    "print(\"hamming_loss: \" +  str(ham_loss))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9SELABqc1okn"
   },
   "outputs": [],
   "source": [
    "binaryRelevanceList.append(round(ham_loss,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "colab_type": "code",
    "id": "OcLiOBIr1okq",
    "outputId": "2f3238e8-bb88-439f-d804-675725f21cbe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 1., 0., ..., 1., 0., 1.],\n",
       "        [1., 0., 0., ..., 0., 0., 1.],\n",
       "        [1., 1., 1., ..., 0., 0., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1., ..., 0., 1., 1.],\n",
       "        [1., 1., 1., ..., 0., 0., 1.],\n",
       "        [1., 1., 0., ..., 1., 0., 1.]],\n",
       "\n",
       "       [[0., 0., 1., ..., 0., 1., 0.],\n",
       "        [0., 1., 1., ..., 1., 1., 0.],\n",
       "        [0., 0., 0., ..., 1., 1., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 1., 0., 0.],\n",
       "        [0., 0., 0., ..., 1., 1., 0.],\n",
       "        [0., 0., 1., ..., 0., 1., 0.]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob = my_model_DT.predict_proba(x_test)\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "blomXWvH1okt"
   },
   "source": [
    "## Task 2: Implement the Binary Relevance Algorithm with Under-Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Od5NKPt1okt"
   },
   "outputs": [],
   "source": [
    "class BinaryRelevanceAlgWithUnderSampling(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    # Constructor for the classifier object\n",
    "    def __init__(self, PredictClassifier=GaussianNB(),underSampling=False):\n",
    "        self.PredictClassifier = PredictClassifier\n",
    "        self.underSampling = underSampling\n",
    "\n",
    "    # The fit function to train a classifier\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        models = []\n",
    "          \n",
    "        #For each label in the data set training a model of the given classifier on all the features \n",
    "        #and storing all individual models in a list.\n",
    "        for instance in y.columns:\n",
    "            X, y[instance] = check_X_y(X, y[instance])\n",
    "            x_resampled = X\n",
    "            y_resampled = y[instance]\n",
    "            #performing undersampling\n",
    "            if self.underSampling == True:\n",
    "                cc = RandomUnderSampler(random_state=0)\n",
    "                x_resampled, y_resampled = cc.fit_resample(x_resampled, y_resampled)\n",
    "            model = copy.copy(self.PredictClassifier)\n",
    "            model.fit(x_resampled,y_resampled)\n",
    "            models.append(model)\n",
    "\n",
    "        self.models_ = models\n",
    "        \n",
    "        # Return the classifier\n",
    "        return self\n",
    "\n",
    "    # The predict function to make a set of predictions for a set of query instances\n",
    "    def predict(self, X):\n",
    "\n",
    "        # Check that the input features match the type and shape of the training features\n",
    "        X = check_array(X)\n",
    "        \n",
    "        # Initialise an empty list to store the predictions made\n",
    "        predictions = list()\n",
    "        \n",
    "        #loop thorugh all models in the list to make the predictions for each label\n",
    "        for model in self.models_:\n",
    "            predict = model.predict(X)\n",
    "            predictions.append(predict)\n",
    "        return np.array(predictions).T\n",
    "    \n",
    "    \n",
    "    # The predict function to make a set of predictions for a set of query instances\n",
    "    def predict_proba(self, X):\n",
    "\n",
    "        # Check that the input features match the type and shape of the training features\n",
    "        X = check_array(X)\n",
    "        \n",
    "         # Initialise an empty list to store the predictions made\n",
    "        predict_proba_list = list()\n",
    "        \n",
    "        #loop thorugh all models in the list to make the predictions and probabilities for each label\n",
    "        for model in self.models_:\n",
    "            predict_pro = model.predict_proba(X)\n",
    "            predict_proba_list.append(predict_pro)\n",
    "            \n",
    "        return np.array(predict_proba_list).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n18AmLXi1okw"
   },
   "source": [
    "## Evaluation of Binary Relevance with Under Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NJ1sbaMo1okw"
   },
   "source": [
    "### Classifier 1 : Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "HNEWnawt1okx",
    "outputId": "a6e4030a-ec16-4db8-d315-37d71ae87e44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinaryRelevanceAlgWithUnderSampling(PredictClassifier=GaussianNB(priors=None,\n",
       "                                                                 var_smoothing=1e-09),\n",
       "                                    underSampling=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_US_NB = BinaryRelevanceAlgWithUnderSampling(GaussianNB(),True)\n",
    "my_model_US_NB.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "colab_type": "code",
    "id": "PEnwK2Zx1ok0",
    "outputId": "06f2ea18-2aae-4c77-8505-cdd41fb0ad48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hamming_loss: 0.3735242030696576\n",
      "\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.66      0.58       143\n",
      "           1       0.52      0.71      0.60       206\n",
      "           2       0.63      0.77      0.69       201\n",
      "           3       0.61      0.63      0.62       180\n",
      "           4       0.53      0.55      0.54       152\n",
      "           5       0.40      0.62      0.49       124\n",
      "           6       0.24      0.56      0.33        90\n",
      "           7       0.28      0.63      0.39       102\n",
      "           8       0.10      0.60      0.17        40\n",
      "           9       0.18      0.75      0.29        56\n",
      "          10       0.20      0.64      0.31        70\n",
      "          11       0.84      0.51      0.63       369\n",
      "          12       0.82      0.52      0.64       364\n",
      "          13       0.03      0.44      0.05         9\n",
      "\n",
      "   micro avg       0.43      0.61      0.50      2106\n",
      "   macro avg       0.42      0.61      0.45      2106\n",
      "weighted avg       0.59      0.61      0.56      2106\n",
      " samples avg       0.45      0.62      0.50      2106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make a set of predictions for the training data\n",
    "y_pred = my_model_US_NB.predict(x_test)\n",
    "\n",
    "ham_loss = hamming_loss(y_test, y_pred)\n",
    "print(\"hamming_loss: \" +  str(ham_loss))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gf-CQNk01ok3"
   },
   "outputs": [],
   "source": [
    "binaryRelevanceWithUndersamplingList.append(round(ham_loss,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "colab_type": "code",
    "id": "73GSk0aR1ok5",
    "outputId": "1f373c25-b0db-4752-a451-f6617cab0ad7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[9.90171848e-01, 9.31739930e-01, 2.35125369e-01, ...,\n",
       "         1.00658751e-02, 6.25679334e-02, 2.76191157e-02],\n",
       "        [3.48565694e-02, 2.90904586e-04, 1.63998320e-03, ...,\n",
       "         9.55729128e-01, 9.96179196e-01, 8.45721178e-01],\n",
       "        [9.62720781e-01, 8.30659429e-01, 1.25442867e-01, ...,\n",
       "         7.91203840e-02, 2.70025351e-02, 9.99198364e-01],\n",
       "        ...,\n",
       "        [8.78605183e-08, 7.85926771e-03, 9.99943422e-01, ...,\n",
       "         9.98022135e-01, 9.99655631e-01, 7.35453494e-01],\n",
       "        [1.96414784e-01, 6.28043346e-02, 6.65864584e-01, ...,\n",
       "         4.29711275e-01, 9.84144622e-02, 1.16401622e-04],\n",
       "        [8.60085374e-01, 1.38008558e-03, 1.21188088e-05, ...,\n",
       "         9.93572547e-01, 6.31223911e-02, 6.84020572e-01]],\n",
       "\n",
       "       [[9.82815186e-03, 6.82600703e-02, 7.64874631e-01, ...,\n",
       "         9.89934125e-01, 9.37432067e-01, 9.72380884e-01],\n",
       "        [9.65143431e-01, 9.99709095e-01, 9.98360017e-01, ...,\n",
       "         4.42708721e-02, 3.82080421e-03, 1.54278822e-01],\n",
       "        [3.72792187e-02, 1.69340571e-01, 8.74557133e-01, ...,\n",
       "         9.20879616e-01, 9.72997465e-01, 8.01636066e-04],\n",
       "        ...,\n",
       "        [9.99999912e-01, 9.92140732e-01, 5.65782233e-05, ...,\n",
       "         1.97786462e-03, 3.44369132e-04, 2.64546506e-01],\n",
       "        [8.03585216e-01, 9.37195665e-01, 3.34135416e-01, ...,\n",
       "         5.70288725e-01, 9.01585538e-01, 9.99883598e-01],\n",
       "        [1.39914626e-01, 9.98619914e-01, 9.99987881e-01, ...,\n",
       "         6.42745331e-03, 9.36877609e-01, 3.15979428e-01]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob = my_model_US_NB.predict_proba(x_test)\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pk3jaSrH1ok8"
   },
   "source": [
    "### Classifier 2 : Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "colab_type": "code",
    "id": "9S-Mpxo01ok9",
    "outputId": "152f3600-7fbf-4801-f4b6-3cfdb2a8e415"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinaryRelevanceAlgWithUnderSampling(PredictClassifier=DecisionTreeClassifier(ccp_alpha=0.0,\n",
       "                                                                             class_weight=None,\n",
       "                                                                             criterion='entropy',\n",
       "                                                                             max_depth=None,\n",
       "                                                                             max_features=None,\n",
       "                                                                             max_leaf_nodes=None,\n",
       "                                                                             min_impurity_decrease=0.0,\n",
       "                                                                             min_impurity_split=None,\n",
       "                                                                             min_samples_leaf=1,\n",
       "                                                                             min_samples_split=2,\n",
       "                                                                             min_weight_fraction_leaf=0.0,\n",
       "                                                                             presort='deprecated',\n",
       "                                                                             random_state=None,\n",
       "                                                                             splitter='best'),\n",
       "                                    underSampling=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_US_DT = BinaryRelevanceAlgWithUnderSampling(tree.DecisionTreeClassifier(criterion=\"entropy\"),True)\n",
    "my_model_US_DT.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "colab_type": "code",
    "id": "sSQjNWy51olA",
    "outputId": "45f10d4c-3c60-4886-dcca-dd1a43505945"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hamming_loss: 0.41278040141676503\n",
      "\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.57      0.45       143\n",
      "           1       0.52      0.63      0.57       206\n",
      "           2       0.56      0.60      0.58       201\n",
      "           3       0.50      0.64      0.57       180\n",
      "           4       0.42      0.61      0.50       152\n",
      "           5       0.33      0.54      0.41       124\n",
      "           6       0.24      0.53      0.33        90\n",
      "           7       0.26      0.59      0.36       102\n",
      "           8       0.09      0.50      0.15        40\n",
      "           9       0.16      0.66      0.26        56\n",
      "          10       0.17      0.61      0.27        70\n",
      "          11       0.77      0.56      0.65       369\n",
      "          12       0.80      0.56      0.66       364\n",
      "          13       0.02      0.33      0.04         9\n",
      "\n",
      "   micro avg       0.39      0.58      0.47      2106\n",
      "   macro avg       0.37      0.57      0.41      2106\n",
      "weighted avg       0.53      0.58      0.53      2106\n",
      " samples avg       0.41      0.59      0.46      2106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make a set of predictions for the training data\n",
    "y_pred = my_model_US_DT.predict(x_test)\n",
    "\n",
    "ham_loss = hamming_loss(y_test, y_pred)\n",
    "print(\"hamming_loss: \" +  str(ham_loss))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zDruPqRv1olC"
   },
   "outputs": [],
   "source": [
    "binaryRelevanceWithUndersamplingList.append(round(ham_loss,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "colab_type": "code",
    "id": "4C5vDwtC1olF",
    "outputId": "be750e1b-1c8e-4d24-a43b-318c7472522a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 0., ..., 0., 1., 0.],\n",
       "        [1., 0., 0., ..., 1., 0., 1.],\n",
       "        ...,\n",
       "        [1., 0., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 0., 0., 0.],\n",
       "        [1., 0., 1., ..., 1., 1., 1.]],\n",
       "\n",
       "       [[0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 1., 0., 1.],\n",
       "        [0., 1., 1., ..., 0., 1., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 1., 1., 1.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob = my_model_US_DT.predict_proba(x_test)\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O3gyix4N1olK"
   },
   "source": [
    "### Classifier 3 : Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "fFj4PsKw1olL",
    "outputId": "470676a8-e262-49d1-df77-76e8f42e2b4c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinaryRelevanceAlgWithUnderSampling(PredictClassifier=RandomForestClassifier(bootstrap=True,\n",
       "                                                                             ccp_alpha=0.0,\n",
       "                                                                             class_weight=None,\n",
       "                                                                             criterion='gini',\n",
       "                                                                             max_depth=None,\n",
       "                                                                             max_features=3,\n",
       "                                                                             max_leaf_nodes=None,\n",
       "                                                                             max_samples=None,\n",
       "                                                                             min_impurity_decrease=0.0,\n",
       "                                                                             min_impurity_split=None,\n",
       "                                                                             min_samples_leaf=1,\n",
       "                                                                             min_samples_split=200,\n",
       "                                                                             min_weight_fraction_leaf=0.0,\n",
       "                                                                             n_estimators=300,\n",
       "                                                                             n_jobs=None,\n",
       "                                                                             oob_score=False,\n",
       "                                                                             random_state=None,\n",
       "                                                                             verbose=0,\n",
       "                                                                             warm_start=False),\n",
       "                                    underSampling=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_US_RF = BinaryRelevanceAlgWithUnderSampling(ensemble.RandomForestClassifier(n_estimators=300, max_features = 3, min_samples_split=200),True)\n",
    "my_model_US_RF.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "colab_type": "code",
    "id": "dU77n69d1olO",
    "outputId": "d744474e-b559-4040-9b09-1778771d876e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hamming_loss: 0.43845926800472257\n",
      "\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.66      0.56       143\n",
      "           1       0.54      0.71      0.61       206\n",
      "           2       0.65      0.77      0.70       201\n",
      "           3       0.65      0.66      0.65       180\n",
      "           4       0.55      0.59      0.57       152\n",
      "           5       0.38      0.58      0.46       124\n",
      "           6       0.27      0.60      0.37        90\n",
      "           7       0.33      0.62      0.43       102\n",
      "           8       0.08      1.00      0.15        40\n",
      "           9       0.17      0.77      0.28        56\n",
      "          10       0.23      0.71      0.34        70\n",
      "          11       0.85      0.57      0.68       369\n",
      "          12       0.82      0.56      0.67       364\n",
      "          13       0.02      1.00      0.04         9\n",
      "\n",
      "   micro avg       0.38      0.64      0.48      2106\n",
      "   macro avg       0.43      0.70      0.47      2106\n",
      "weighted avg       0.60      0.64      0.58      2106\n",
      " samples avg       0.38      0.65      0.46      2106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make a set of predictions for the training data\n",
    "y_pred = my_model_US_RF.predict(x_test)\n",
    "\n",
    "ham_loss = hamming_loss(y_test, y_pred)\n",
    "print(\"hamming_loss: \" +  str(ham_loss))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LpARYGaW1olQ"
   },
   "outputs": [],
   "source": [
    "binaryRelevanceWithUndersamplingList.append(round(ham_loss,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "colab_type": "code",
    "id": "Q9VoBkg51olS",
    "outputId": "7a00a37a-efb0-4d34-e0f5-e598e1575433"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.56521496, 0.53412908, 0.50556253, ..., 0.45329994,\n",
       "         0.4603608 , 0.4978    ],\n",
       "        [0.52320077, 0.4103086 , 0.44185928, ..., 0.4975778 ,\n",
       "         0.51455576, 0.4978    ],\n",
       "        [0.5226242 , 0.50502037, 0.50838255, ..., 0.43101106,\n",
       "         0.43678729, 0.4978    ],\n",
       "        ...,\n",
       "        [0.37723562, 0.45171641, 0.59443111, ..., 0.54682142,\n",
       "         0.57588881, 0.4978    ],\n",
       "        [0.52782638, 0.48998022, 0.50500209, ..., 0.49781747,\n",
       "         0.49130303, 0.4978    ],\n",
       "        [0.5771484 , 0.46729177, 0.36747013, ..., 0.58814761,\n",
       "         0.53702826, 0.4978    ]],\n",
       "\n",
       "       [[0.43478504, 0.46587092, 0.49443747, ..., 0.54670006,\n",
       "         0.5396392 , 0.5022    ],\n",
       "        [0.47679923, 0.5896914 , 0.55814072, ..., 0.5024222 ,\n",
       "         0.48544424, 0.5022    ],\n",
       "        [0.4773758 , 0.49497963, 0.49161745, ..., 0.56898894,\n",
       "         0.56321271, 0.5022    ],\n",
       "        ...,\n",
       "        [0.62276438, 0.54828359, 0.40556889, ..., 0.45317858,\n",
       "         0.42411119, 0.5022    ],\n",
       "        [0.47217362, 0.51001978, 0.49499791, ..., 0.50218253,\n",
       "         0.50869697, 0.5022    ],\n",
       "        [0.4228516 , 0.53270823, 0.63252987, ..., 0.41185239,\n",
       "         0.46297174, 0.5022    ]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob = my_model_US_RF.predict_proba(x_test)\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qSM8ATx71olV"
   },
   "source": [
    "### Classifier 4 : Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "4Yeap2Wp1olW",
    "outputId": "aa8e45ab-35fd-4884-85d6-1001df5a0da8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinaryRelevanceAlgWithUnderSampling(PredictClassifier=LogisticRegression(C=1.0,\n",
       "                                                                         class_weight=None,\n",
       "                                                                         dual=False,\n",
       "                                                                         fit_intercept=True,\n",
       "                                                                         intercept_scaling=1,\n",
       "                                                                         l1_ratio=None,\n",
       "                                                                         max_iter=100,\n",
       "                                                                         multi_class='auto',\n",
       "                                                                         n_jobs=None,\n",
       "                                                                         penalty='l2',\n",
       "                                                                         random_state=None,\n",
       "                                                                         solver='lbfgs',\n",
       "                                                                         tol=0.0001,\n",
       "                                                                         verbose=0,\n",
       "                                                                         warm_start=False),\n",
       "                                    underSampling=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_US_LR = BinaryRelevanceAlgWithUnderSampling(linear_model.LogisticRegression(),True)\n",
    "my_model_US_LR.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "colab_type": "code",
    "id": "aE6L8ukH1ola",
    "outputId": "6378cc78-a5ed-43e4-c648-44e6f3d22d40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hamming_loss: 0.36038961038961037\n",
      "\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.66      0.59       143\n",
      "           1       0.54      0.66      0.59       206\n",
      "           2       0.66      0.77      0.71       201\n",
      "           3       0.59      0.71      0.65       180\n",
      "           4       0.50      0.62      0.55       152\n",
      "           5       0.39      0.62      0.48       124\n",
      "           6       0.26      0.62      0.37        90\n",
      "           7       0.25      0.55      0.35       102\n",
      "           8       0.11      0.57      0.19        40\n",
      "           9       0.19      0.70      0.30        56\n",
      "          10       0.20      0.60      0.30        70\n",
      "          11       0.84      0.55      0.67       369\n",
      "          12       0.83      0.56      0.67       364\n",
      "          13       0.03      0.56      0.06         9\n",
      "\n",
      "   micro avg       0.44      0.62      0.52      2106\n",
      "   macro avg       0.42      0.63      0.46      2106\n",
      "weighted avg       0.59      0.62      0.58      2106\n",
      " samples avg       0.46      0.63      0.51      2106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make a set of predictions for the training data\n",
    "y_pred = my_model_US_LR.predict(x_test)\n",
    "\n",
    "ham_loss = hamming_loss(y_test, y_pred)\n",
    "print(\"hamming_loss: \" +  str(ham_loss))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "43MavKXS1old"
   },
   "outputs": [],
   "source": [
    "binaryRelevanceWithUndersamplingList.append(round(ham_loss,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "colab_type": "code",
    "id": "O_rQA54B1olf",
    "outputId": "80dbaefb-3a74-4d0f-c444-7922ba16828f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.68964122, 0.76797283, 0.61232977, ..., 0.4514248 ,\n",
       "         0.4469043 , 0.49815549],\n",
       "        [0.60223981, 0.48236878, 0.28689987, ..., 0.61600276,\n",
       "         0.67164209, 0.48057672],\n",
       "        [0.63637805, 0.35085557, 0.45870198, ..., 0.31203222,\n",
       "         0.40115888, 0.6272534 ],\n",
       "        ...,\n",
       "        [0.18973715, 0.27232406, 0.66247796, ..., 0.5476897 ,\n",
       "         0.63257763, 0.67372514],\n",
       "        [0.74921949, 0.56659706, 0.37983968, ..., 0.53718317,\n",
       "         0.45701152, 0.58643042],\n",
       "        [0.60748455, 0.4142099 , 0.16524912, ..., 0.73048552,\n",
       "         0.53708431, 0.53239373]],\n",
       "\n",
       "       [[0.31035878, 0.23202717, 0.38767023, ..., 0.5485752 ,\n",
       "         0.5530957 , 0.50184451],\n",
       "        [0.39776019, 0.51763122, 0.71310013, ..., 0.38399724,\n",
       "         0.32835791, 0.51942328],\n",
       "        [0.36362195, 0.64914443, 0.54129802, ..., 0.68796778,\n",
       "         0.59884112, 0.3727466 ],\n",
       "        ...,\n",
       "        [0.81026285, 0.72767594, 0.33752204, ..., 0.4523103 ,\n",
       "         0.36742237, 0.32627486],\n",
       "        [0.25078051, 0.43340294, 0.62016032, ..., 0.46281683,\n",
       "         0.54298848, 0.41356958],\n",
       "        [0.39251545, 0.5857901 , 0.83475088, ..., 0.26951448,\n",
       "         0.46291569, 0.46760627]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob = my_model_US_LR.predict_proba(x_test)\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AwJMtDoL1oli"
   },
   "source": [
    "## Task 3: Compare the Performance of Different Binary Relevance Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aJToDVFr1oli"
   },
   "outputs": [],
   "source": [
    "cv_folds=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i-NA3r-H1olk"
   },
   "source": [
    "#### Binary Relevance Algorithm with Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "rV4FipDA1olk",
    "outputId": "025985e8-2bbd-4ba1-f432-8a49744188fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2757732  0.29381443 0.31369661 0.30014804 0.30792006 0.27942265\n",
      " 0.31828275 0.32050333 0.28793486 0.31791266]\n"
     ]
    }
   ],
   "source": [
    "my_model_BR_US = BinaryRelevanceAlgWithUnderSampling()\n",
    "#Calculating Cross-Validation Scores\n",
    "scores = cross_val_score(my_model_BR_US, X_train, y_train, cv=cv_folds, n_jobs=-1,scoring = make_scorer(hamming_loss))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "hZfxgzPY1oln",
    "outputId": "37bb9b10-fb00-4e39-edc3-e14da94ecc44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PredictClassifier': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False), 'underSampling': False}"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-0.20364436184077517"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up the parameter grid to seaerch\n",
    "param_grid = {'PredictClassifier': [GaussianNB(), tree.DecisionTreeClassifier(criterion=\"entropy\") ,\\\n",
    "                                   ensemble.RandomForestClassifier(n_estimators=300, max_features = 3, min_samples_split=200), \\\n",
    "                                   linear_model.LogisticRegression()],\\\n",
    "               'underSampling':[True,False]}\n",
    "\n",
    "# Perform the search\n",
    "my_tuned_tree_BR_US = GridSearchCV(BinaryRelevanceAlgWithUnderSampling(), \\\n",
    "                                param_grid, cv=2, verbose = 0, \\\n",
    "                            return_train_score=True,scoring = make_scorer(hamming_loss,greater_is_better=False))\n",
    "my_tuned_tree_BR_US.fit(X_train, y_train)\n",
    "\n",
    "# Print details\n",
    "print(\"Best parameters set found on development set:\")\n",
    "display(my_tuned_tree_BR_US.best_params_)\n",
    "display(my_tuned_tree_BR_US.best_score_)\n",
    "# display(my_tuned_tree_BR_US.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RotIKN4D1olq"
   },
   "source": [
    "#### Binary Relevance Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "JiOR-I8j1olr",
    "outputId": "9e1ac7fb-0723-4254-f305-5e852ca082a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2757732  0.29381443 0.31369661 0.30014804 0.30792006 0.27942265\n",
      " 0.31828275 0.32050333 0.28793486 0.31791266]\n"
     ]
    }
   ],
   "source": [
    "my_model_BR = BinaryRelevanceAlg()\n",
    "#Calculating Cross-Validation Scores\n",
    "scores = cross_val_score(my_model_BR, X_train, y_train, cv=cv_folds, n_jobs=-1,scoring = make_scorer(hamming_loss))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "WEoUw4Wu1olt",
    "outputId": "76a1b6c3-8525-4861-fc94-00109d591e3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PredictClassifier': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False)}"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-0.20364436184077517"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up the parameter grid to seaerch\n",
    "param_grid = {'PredictClassifier': [GaussianNB(), tree.DecisionTreeClassifier(criterion=\"entropy\") ,\\\n",
    "                                   ensemble.RandomForestClassifier(n_estimators=300, max_features = 3, min_samples_split=200), \\\n",
    "                                   linear_model.LogisticRegression()]}\n",
    "\n",
    "# Perform the search\n",
    "my_tuned_tree_BR = GridSearchCV(BinaryRelevanceAlg(), \\\n",
    "                                param_grid, cv=2, verbose = 0, \\\n",
    "                            return_train_score=True,scoring = make_scorer(hamming_loss,greater_is_better=False))\n",
    "my_tuned_tree_BR.fit(X_train, y_train)\n",
    "\n",
    "# Print details\n",
    "print(\"Best parameters set found on development set:\")\n",
    "display(my_tuned_tree_BR.best_params_)\n",
    "display(my_tuned_tree_BR.best_score_)\n",
    "# display(my_tuned_tree_BR.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SrJzPqBt1olv"
   },
   "source": [
    "## Task 4: Implement the Classifier Chains Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XMaF8VWf1olw"
   },
   "outputs": [],
   "source": [
    "class ClassifierChainsAlg(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    # Constructor for the classifier object\n",
    "    def __init__(self, PredictClassifier=GaussianNB(),underSampling=False):\n",
    "        self.PredictClassifier = PredictClassifier\n",
    "        self.underSampling = underSampling\n",
    "\n",
    "    # The fit function to train a classifier\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        models = []\n",
    "        \n",
    "        #For each label in the data set training a model of the given classifier on all the features \n",
    "        #and storing all individual models in a list.\n",
    "        for i in range(len(y.columns)):\n",
    "            #combining all the previous labels to the feature set\n",
    "            x_new = X.join(y.iloc[:,:i])\n",
    "            y_new = y.iloc[:,i]\n",
    "            x_new,y_new = check_X_y(x_new, y_new)\n",
    "            x_resampled = x_new\n",
    "            y_resampled = y_new\n",
    "            #performing undersampling\n",
    "            if self.underSampling == True:\n",
    "                cc = RandomUnderSampler(random_state=0)\n",
    "                x_resampled, y_resampled = cc.fit_resample(x_resampled, y_resampled)\n",
    "            model = copy.copy(self.PredictClassifier)\n",
    "            model.fit(x_resampled,y_resampled)\n",
    "            models.append(model)\n",
    "        \n",
    "        self.models_ = models\n",
    "        \n",
    "        # Return the classifier\n",
    "        return self\n",
    "\n",
    "    # The predict function to make a set of predictions for a set of query instances\n",
    "    def predict(self, X):\n",
    "\n",
    "        # Check that the input features match the type and shape of the training features\n",
    "        x_new = check_array(X)\n",
    "        \n",
    "        predictions = list()\n",
    "        \n",
    "        #loop thorugh all models in the list to make the predictions for each label\n",
    "        for model in self.models_:\n",
    "            predict = model.predict(x_new)\n",
    "            predictions.append(predict)\n",
    "            x_new = np.concatenate((x_new,predict[:,None]),axis=1)\n",
    "        return np.array(predictions).T\n",
    "    \n",
    "    \n",
    "    # The predict function to make a set of predictions for a set of query instances\n",
    "    def predict_proba(self, X):\n",
    "\n",
    "        # Check that the input features match the type and shape of the training features\n",
    "        x_new = check_array(X)\n",
    "        \n",
    "         # Initialise an empty list to store the predictions made\n",
    "        predict_proba_list = list()\n",
    "        \n",
    "        #loop thorugh all models in the list to make the predictions and probabilities for each label\n",
    "        for model in self.models_:\n",
    "            predict_pro = model.predict_proba(x_new)\n",
    "            predict_proba_list.append(predict_pro)\n",
    "            #Getting the predict to join and make the features for next predict proba\n",
    "            predict = model.predict(x_new)\n",
    "            x_new = np.concatenate((x_new,predict[:,None]),axis=1)\n",
    "            \n",
    "        return np.array(predict_proba_list).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X0ebQr231olz"
   },
   "source": [
    "## Evaluation of Classifier Chains Alogrithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ff32ssUQ1ol0"
   },
   "source": [
    "### Classifier 1 : Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "tFy0RazN1ol0",
    "outputId": "d471589e-539a-46a5-871e-183375cd573f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassifierChainsAlg(PredictClassifier=GaussianNB(priors=None,\n",
       "                                                 var_smoothing=1e-09),\n",
       "                    underSampling=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_CC_NB = ClassifierChainsAlg(GaussianNB())\n",
    "my_model_CC_NB.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "colab_type": "code",
    "id": "20Wn-jnI1ol2",
    "outputId": "8fdcd9cd-c0a5-4510-d3ea-e697cd0eef6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hamming_loss: 0.313754427390791\n",
      "\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.65      0.60       143\n",
      "           1       0.52      0.67      0.58       206\n",
      "           2       0.66      0.75      0.70       201\n",
      "           3       0.60      0.61      0.60       180\n",
      "           4       0.58      0.50      0.54       152\n",
      "           5       0.43      0.44      0.43       124\n",
      "           6       0.30      0.43      0.35        90\n",
      "           7       0.31      0.40      0.35       102\n",
      "           8       0.08      0.23      0.12        40\n",
      "           9       0.18      0.52      0.27        56\n",
      "          10       0.20      0.50      0.28        70\n",
      "          11       0.82      0.56      0.66       369\n",
      "          12       0.81      0.56      0.66       364\n",
      "          13       0.05      0.44      0.08         9\n",
      "\n",
      "   micro avg       0.50      0.56      0.53      2106\n",
      "   macro avg       0.44      0.52      0.45      2106\n",
      "weighted avg       0.59      0.56      0.56      2106\n",
      " samples avg       0.51      0.57      0.52      2106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make a set of predictions for the training data\n",
    "y_pred = my_model_CC_NB.predict(x_test)\n",
    "\n",
    "ham_loss = hamming_loss(y_test, y_pred)\n",
    "print(\"hamming_loss: \" +  str(ham_loss))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K19AvshC1ol4"
   },
   "outputs": [],
   "source": [
    "chainClassificationList.append(round(ham_loss,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "colab_type": "code",
    "id": "pyO-zi0r1ol6",
    "outputId": "25fb6f33-b0c6-4ff5-d43d-8aa0b35028ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[9.96639774e-01, 9.88868868e-01, 5.21347190e-01, ...,\n",
       "         7.62460356e-04, 7.43449217e-17, 1.00000000e+00],\n",
       "        [1.07392885e-01, 8.03480717e-06, 6.84791728e-02, ...,\n",
       "         9.99274068e-01, 1.00000000e+00, 1.00000000e+00],\n",
       "        [9.87501495e-01, 9.25354954e-01, 1.28948047e-01, ...,\n",
       "         1.48882502e-03, 2.48561143e-16, 2.41401803e-20],\n",
       "        ...,\n",
       "        [2.12813779e-07, 2.15023164e-04, 9.99976459e-01, ...,\n",
       "         9.99901356e-01, 1.00000000e+00, 1.00000000e+00],\n",
       "        [1.12346569e-01, 2.51405080e-03, 8.78496524e-01, ...,\n",
       "         5.95079447e-01, 1.00000000e+00, 1.00000000e+00],\n",
       "        [9.92305132e-01, 8.68962424e-03, 1.23264422e-05, ...,\n",
       "         9.81606660e-01, 1.00000000e+00, 1.00000000e+00]],\n",
       "\n",
       "       [[3.36022634e-03, 1.11311317e-02, 4.78652810e-01, ...,\n",
       "         9.99237540e-01, 1.00000000e+00, 0.00000000e+00],\n",
       "        [8.92607115e-01, 9.99991965e-01, 9.31520827e-01, ...,\n",
       "         7.25932295e-04, 0.00000000e+00, 0.00000000e+00],\n",
       "        [1.24985049e-02, 7.46450465e-02, 8.71051953e-01, ...,\n",
       "         9.98511175e-01, 1.00000000e+00, 1.00000000e+00],\n",
       "        ...,\n",
       "        [9.99999787e-01, 9.99784977e-01, 2.35412661e-05, ...,\n",
       "         9.86438963e-05, 0.00000000e+00, 0.00000000e+00],\n",
       "        [8.87653431e-01, 9.97485949e-01, 1.21503476e-01, ...,\n",
       "         4.04920553e-01, 0.00000000e+00, 0.00000000e+00],\n",
       "        [7.69486818e-03, 9.91310376e-01, 9.99987674e-01, ...,\n",
       "         1.83933399e-02, 0.00000000e+00, 0.00000000e+00]]])"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob = my_model_CC_NB.predict_proba(x_test)\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h8l2XFRT1ol8"
   },
   "source": [
    "### Classifier 2 : Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "colab_type": "code",
    "id": "ki6qoiEO1ol8",
    "outputId": "de1116df-539f-4b8e-9e2b-49e50412923b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassifierChainsAlg(PredictClassifier=DecisionTreeClassifier(ccp_alpha=0.0,\n",
       "                                                             class_weight=None,\n",
       "                                                             criterion='entropy',\n",
       "                                                             max_depth=None,\n",
       "                                                             max_features=None,\n",
       "                                                             max_leaf_nodes=None,\n",
       "                                                             min_impurity_decrease=0.0,\n",
       "                                                             min_impurity_split=None,\n",
       "                                                             min_samples_leaf=1,\n",
       "                                                             min_samples_split=2,\n",
       "                                                             min_weight_fraction_leaf=0.0,\n",
       "                                                             presort='deprecated',\n",
       "                                                             random_state=None,\n",
       "                                                             splitter='best'),\n",
       "                    underSampling=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_CC_DT = ClassifierChainsAlg(tree.DecisionTreeClassifier(criterion=\"entropy\"))\n",
    "my_model_CC_DT.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "colab_type": "code",
    "id": "D2YkYaHX1ol-",
    "outputId": "73ae9f04-a607-47ef-bdae-e6a25fbe98bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hamming_loss: 0.269185360094451\n",
      "\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.55      0.53       143\n",
      "           1       0.50      0.50      0.50       206\n",
      "           2       0.58      0.53      0.55       201\n",
      "           3       0.51      0.47      0.49       180\n",
      "           4       0.53      0.47      0.50       152\n",
      "           5       0.42      0.40      0.41       124\n",
      "           6       0.43      0.44      0.44        90\n",
      "           7       0.36      0.36      0.36       102\n",
      "           8       0.18      0.17      0.18        40\n",
      "           9       0.14      0.14      0.14        56\n",
      "          10       0.15      0.14      0.15        70\n",
      "          11       0.79      0.78      0.79       369\n",
      "          12       0.79      0.78      0.78       364\n",
      "          13       0.00      0.00      0.00         9\n",
      "\n",
      "   micro avg       0.57      0.55      0.56      2106\n",
      "   macro avg       0.42      0.41      0.41      2106\n",
      "weighted avg       0.57      0.55      0.56      2106\n",
      " samples avg       0.57      0.56      0.53      2106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make a set of predictions for the training data\n",
    "y_pred = my_model_CC_DT.predict(x_test)\n",
    "\n",
    "ham_loss = hamming_loss(y_test, y_pred)\n",
    "print(\"hamming_loss: \" +  str(ham_loss))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-8g9eaAb1omA"
   },
   "outputs": [],
   "source": [
    "chainClassificationList.append(round(ham_loss,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "colab_type": "code",
    "id": "cyf771fm1omD",
    "outputId": "7656580c-c2ef-47be-c891-df04b85241b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0., 0., ..., 0., 0., 1.],\n",
       "        [1., 0., 0., ..., 0., 0., 1.],\n",
       "        [1., 1., 1., ..., 0., 0., 1.],\n",
       "        ...,\n",
       "        [0., 0., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 0., ..., 0., 0., 1.],\n",
       "        [1., 1., 0., ..., 0., 0., 1.]],\n",
       "\n",
       "       [[0., 1., 1., ..., 1., 1., 0.],\n",
       "        [0., 1., 1., ..., 1., 1., 0.],\n",
       "        [0., 0., 0., ..., 1., 1., 0.],\n",
       "        ...,\n",
       "        [1., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 1., 1., 0.],\n",
       "        [0., 0., 1., ..., 1., 1., 0.]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob = my_model_CC_DT.predict_proba(x_test)\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "41qqmkN_1omG"
   },
   "source": [
    "### Classifier 3 : Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "b3ra2t_41omG",
    "outputId": "62ab008c-00a8-4792-da74-4557d3e4d393"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassifierChainsAlg(PredictClassifier=RandomForestClassifier(bootstrap=True,\n",
       "                                                             ccp_alpha=0.0,\n",
       "                                                             class_weight=None,\n",
       "                                                             criterion='gini',\n",
       "                                                             max_depth=None,\n",
       "                                                             max_features=3,\n",
       "                                                             max_leaf_nodes=None,\n",
       "                                                             max_samples=None,\n",
       "                                                             min_impurity_decrease=0.0,\n",
       "                                                             min_impurity_split=None,\n",
       "                                                             min_samples_leaf=1,\n",
       "                                                             min_samples_split=200,\n",
       "                                                             min_weight_fraction_leaf=0.0,\n",
       "                                                             n_estimators=300,\n",
       "                                                             n_jobs=None,\n",
       "                                                             oob_score=False,\n",
       "                                                             random_state=None,\n",
       "                                                             verbose=0,\n",
       "                                                             warm_start=False),\n",
       "                    underSampling=False)"
      ]
     },
     "execution_count": 55,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_CC_RF = ClassifierChainsAlg(ensemble.RandomForestClassifier(n_estimators=300, max_features = 3, min_samples_split=200))\n",
    "my_model_CC_RF.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "colab_type": "code",
    "id": "XzkmjMW21omJ",
    "outputId": "ab97374b-4669-4c43-da9b-3dbb99e800d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hamming_loss: 0.2139905548996458\n",
      "\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.20      0.32       143\n",
      "           1       0.67      0.19      0.30       206\n",
      "           2       0.78      0.33      0.47       201\n",
      "           3       0.92      0.24      0.39       180\n",
      "           4       0.96      0.17      0.29       152\n",
      "           5       0.00      0.00      0.00       124\n",
      "           6       0.00      0.00      0.00        90\n",
      "           7       0.00      0.00      0.00       102\n",
      "           8       0.00      0.00      0.00        40\n",
      "           9       0.00      0.00      0.00        56\n",
      "          10       0.00      0.00      0.00        70\n",
      "          11       0.76      1.00      0.87       369\n",
      "          12       0.75      1.00      0.86       364\n",
      "          13       0.00      0.00      0.00         9\n",
      "\n",
      "   micro avg       0.77      0.44      0.56      2106\n",
      "   macro avg       0.41      0.22      0.25      2106\n",
      "weighted avg       0.61      0.44      0.45      2106\n",
      " samples avg       0.76      0.44      0.53      2106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make a set of predictions for the training data\n",
    "y_pred = my_model_CC_RF.predict(x_test)\n",
    "\n",
    "ham_loss = hamming_loss(y_test, y_pred)\n",
    "print(\"hamming_loss: \" +  str(ham_loss))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wAvGKvep1omL"
   },
   "outputs": [],
   "source": [
    "chainClassificationList.append(round(ham_loss,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "colab_type": "code",
    "id": "JRUgx4SZ1omN",
    "outputId": "dac10b02-a716-4da3-de61-4a66ed352e8d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.73221839, 0.63603555, 0.59942935, ..., 0.20980818,\n",
       "         0.20658134, 0.97398562],\n",
       "        [0.67436896, 0.52534026, 0.53646678, ..., 0.26578557,\n",
       "         0.23317412, 0.99190523],\n",
       "        [0.69210081, 0.57167589, 0.60096445, ..., 0.22405084,\n",
       "         0.20163845, 0.98950495],\n",
       "        ...,\n",
       "        [0.57439403, 0.54274   , 0.67382764, ..., 0.30980004,\n",
       "         0.26902397, 0.99158596],\n",
       "        [0.67512445, 0.58020792, 0.588319  , ..., 0.27297128,\n",
       "         0.2529769 , 0.98633246],\n",
       "        [0.75076085, 0.55221887, 0.46913726, ..., 0.28316998,\n",
       "         0.2429788 , 0.9895222 ]],\n",
       "\n",
       "       [[0.26778161, 0.36396445, 0.40057065, ..., 0.79019182,\n",
       "         0.79341866, 0.02601438],\n",
       "        [0.32563104, 0.47465974, 0.46353322, ..., 0.73421443,\n",
       "         0.76682588, 0.00809477],\n",
       "        [0.30789919, 0.42832411, 0.39903555, ..., 0.77594916,\n",
       "         0.79836155, 0.01049505],\n",
       "        ...,\n",
       "        [0.42560597, 0.45726   , 0.32617236, ..., 0.69019996,\n",
       "         0.73097603, 0.00841404],\n",
       "        [0.32487555, 0.41979208, 0.411681  , ..., 0.72702872,\n",
       "         0.7470231 , 0.01366754],\n",
       "        [0.24923915, 0.44778113, 0.53086274, ..., 0.71683002,\n",
       "         0.7570212 , 0.0104778 ]]])"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob = my_model_CC_RF.predict_proba(x_test)\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rkUJFtIW1omP"
   },
   "source": [
    "### Classifier 4 : Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "colab_type": "code",
    "id": "ZfoxQ7iN1omQ",
    "outputId": "e3a698d2-599a-454f-b207-a261e4338192"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassifierChainsAlg(PredictClassifier=LogisticRegression(C=1.0,\n",
       "                                                         class_weight=None,\n",
       "                                                         dual=False,\n",
       "                                                         fit_intercept=True,\n",
       "                                                         intercept_scaling=1,\n",
       "                                                         l1_ratio=None,\n",
       "                                                         max_iter=100,\n",
       "                                                         multi_class='auto',\n",
       "                                                         n_jobs=None,\n",
       "                                                         penalty='l2',\n",
       "                                                         random_state=None,\n",
       "                                                         solver='lbfgs',\n",
       "                                                         tol=0.0001, verbose=0,\n",
       "                                                         warm_start=False),\n",
       "                    underSampling=False)"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_CC_LR = ClassifierChainsAlg(linear_model.LogisticRegression())\n",
    "my_model_CC_LR.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "colab_type": "code",
    "id": "8Tnhzo271omR",
    "outputId": "c0eb5caa-f333-450c-fa9a-1a09e0ea7f3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hamming_loss: 0.21369539551357733\n",
      "\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.48      0.56       143\n",
      "           1       0.57      0.38      0.46       206\n",
      "           2       0.72      0.48      0.57       201\n",
      "           3       0.68      0.53      0.60       180\n",
      "           4       0.60      0.58      0.59       152\n",
      "           5       0.48      0.41      0.44       124\n",
      "           6       0.53      0.22      0.31        90\n",
      "           7       0.47      0.18      0.26       102\n",
      "           8       0.00      0.00      0.00        40\n",
      "           9       0.67      0.04      0.07        56\n",
      "          10       0.67      0.03      0.05        70\n",
      "          11       0.78      0.91      0.84       369\n",
      "          12       0.76      0.91      0.83       364\n",
      "          13       0.00      0.00      0.00         9\n",
      "\n",
      "   micro avg       0.69      0.56      0.62      2106\n",
      "   macro avg       0.54      0.37      0.40      2106\n",
      "weighted avg       0.65      0.56      0.58      2106\n",
      " samples avg       0.65      0.56      0.58      2106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make a set of predictions for the training data\n",
    "y_pred = my_model_CC_LR.predict(x_test)\n",
    "\n",
    "ham_loss = hamming_loss(y_test, y_pred)\n",
    "print(\"hamming_loss: \" +  str(ham_loss))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1cgEpyw31omV"
   },
   "outputs": [],
   "source": [
    "chainClassificationList.append(round(ham_loss,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "colab_type": "code",
    "id": "v6ED763C1omX",
    "outputId": "af375ca8-2a66-484d-b26a-a530d921a7cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[8.36208563e-01, 9.25446167e-01, 7.18527838e-01, ...,\n",
       "         1.84386886e-01, 7.13149229e-03, 9.99426389e-01],\n",
       "        [7.59646258e-01, 7.28114161e-01, 5.57281789e-01, ...,\n",
       "         4.94322720e-01, 1.11437883e-02, 9.97446030e-01],\n",
       "        [7.65529174e-01, 5.32050030e-01, 7.19147832e-01, ...,\n",
       "         1.91537883e-01, 8.89998823e-03, 9.99322501e-01],\n",
       "        ...,\n",
       "        [2.83196602e-01, 9.92717434e-02, 7.98500955e-01, ...,\n",
       "         4.88986699e-01, 1.15568934e-02, 9.99892386e-01],\n",
       "        [8.32736594e-01, 7.32283334e-01, 5.53968356e-01, ...,\n",
       "         2.64771273e-01, 9.33098556e-03, 9.99501562e-01],\n",
       "        [7.91222422e-01, 6.19408365e-01, 3.48880254e-01, ...,\n",
       "         2.46458101e-01, 9.01431966e-03, 9.36948872e-01]],\n",
       "\n",
       "       [[1.63791437e-01, 7.45538333e-02, 2.81472162e-01, ...,\n",
       "         8.15613114e-01, 9.92868508e-01, 5.73611228e-04],\n",
       "        [2.40353742e-01, 2.71885839e-01, 4.42718211e-01, ...,\n",
       "         5.05677280e-01, 9.88856212e-01, 2.55396992e-03],\n",
       "        [2.34470826e-01, 4.67949970e-01, 2.80852168e-01, ...,\n",
       "         8.08462117e-01, 9.91100012e-01, 6.77498763e-04],\n",
       "        ...,\n",
       "        [7.16803398e-01, 9.00728257e-01, 2.01499045e-01, ...,\n",
       "         5.11013301e-01, 9.88443107e-01, 1.07613961e-04],\n",
       "        [1.67263406e-01, 2.67716666e-01, 4.46031644e-01, ...,\n",
       "         7.35228727e-01, 9.90669014e-01, 4.98437697e-04],\n",
       "        [2.08777578e-01, 3.80591635e-01, 6.51119746e-01, ...,\n",
       "         7.53541899e-01, 9.90985680e-01, 6.30511276e-02]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob = my_model_CC_LR.predict_proba(x_test)\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RRm_fEkK1omY"
   },
   "source": [
    "## Task 5: Evaluate the Performance of the Classifier Chains Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "eTSznNpC1omZ",
    "outputId": "76de16da-4749-4141-f2b6-302313bffd46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.28055965 0.3011782  0.32658321 0.30940044 0.33049593 0.30088823\n",
      " 0.33456699 0.34974093 0.30495929 0.33086603]\n"
     ]
    }
   ],
   "source": [
    "my_model_BR = ClassifierChainsAlg()\n",
    "#Calculating Cross-Validation Scores\n",
    "scores = cross_val_score(my_model_BR, X_train, y_train, cv=cv_folds, n_jobs=-1,scoring = make_scorer(hamming_loss))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "PhtfMYbY1omb",
    "outputId": "25b1ccc1-b28e-4baa-cc72-719c2f6bf23f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PredictClassifier': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=None, max_features=3,\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=200,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=300,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False), 'underSampling': False}"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-0.21694546934371067"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up the parameter grid to seaerch\n",
    "param_grid = {'PredictClassifier': [GaussianNB(), tree.DecisionTreeClassifier(criterion=\"entropy\") ,\\\n",
    "                                   ensemble.RandomForestClassifier(n_estimators=300, max_features = 3, min_samples_split=200), \\\n",
    "                                   linear_model.LogisticRegression()],\\\n",
    "               'underSampling':[True,False]}\n",
    "\n",
    "# Perform the search\n",
    "my_tuned_tree_CC = GridSearchCV(ClassifierChainsAlg(), \\\n",
    "                                param_grid, cv=2, verbose = 0, \\\n",
    "                            return_train_score=True,scoring = make_scorer(hamming_loss,greater_is_better=False))\n",
    "my_tuned_tree_CC.fit(X_train, y_train)\n",
    "\n",
    "# Print details\n",
    "print(\"Best parameters set found on development set:\")\n",
    "display(my_tuned_tree_CC.best_params_)\n",
    "display(my_tuned_tree_CC.best_score_)\n",
    "# display(my_tuned_tree_CC.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZvXwTItz1omd"
   },
   "source": [
    "## Graphs for each of the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "v3Rcg69l1omd",
    "outputId": "4fb39f7c-d4c0-498c-e694-8295b843ed2e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Binary Relevance')"
      ]
     },
     "execution_count": 65,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcAAAAFNCAYAAACXC791AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3debxdVX338c+XMClBxnRiMKg4ICqt\nAdtqUSsiSiu2jwM4ocWmaNEqYsVaKaLtg1pbq4UKVQQHxIGWRk2LKKJWRZPIGFpqGBQoT0VBhDIG\nfs8fe11zuN4kJ8nduUn25/16ndfde+3pd/Y95/zOWnuftVJVSJI0NJvNdACSJM0EE6AkaZBMgJKk\nQTIBSpIGyQQoSRokE6AkaZBMgNIYknwwydtmOo51leT4JB+f6TikDYEJUAKSXJvkziS3J7klyReS\n7DaxvKqOrKp3zGSMAEnmJqkW5+0t7mNnOi5pY2QClFb43aqaDfwy8D/AB/o+YJLN13LT7Vuszwfe\nluSZ0xiWNAgmQGmSqroL+Cyw10RZktOTvLNNPy3J9UnemOSHSW5M8sqRdQ9OclGSnya5LsnxI8sm\nanBHJPkBcH6rbb52NIYklyb5vTFiXQwsBfYZ2fZXkpyd5KYk1yR53cq2T/LrSb6Z5CdJLknytFb+\noiSLJ637hiQL1uA5Hp7kB0l+lOStI8tnJfmzJFcluS3JkonadpJHJzkvyc1JrkzywtWdA2ltmQCl\nSZI8GHgRcOEqVvslYDtgF+AI4KQkO7Rl/wu8HNgeOBh4dZLnTdr+qcBjgGcBZwAvHTn+E9p+vzBG\nrL8O7A0sa/ObAZ8DLmn7eAbw+iTPmmLbiWO8E9gROAY4O8mcto9HJdlzZJMXA2euwXN8CvCoFsNx\nSR7Tyo8GDgOeAzwE+APgjiTbAOe1Y/wCcChwcpK9kHpgApRWOCfJT4BbgWcC71nFuvcCJ1TVvVW1\nELid7sOeqrqgqi6rqvur6lLgk3QJb9TxVfW/VXUnsAB45EiyeRnwqaq6ZxXH/1GSO4FvAScD57Ty\nfYE5VXVCVd1TVVcD/0iXTCZ7KbCwqha2WM8DFgPPqao7gH+hS1S02B7dYh33Ob69qu6sqkvoEvIT\nWvmrgD+vqiurc0lV/Rj4HeDaqvpIVS2vqouAs4EXrOI8SGvNBCit8Lyq2h7YGjgK+GqSX1rJuj+u\nquUj83cAswGSPCnJV1oT5K3AkcDOk7a/bmKiNbl+Cnhpq8EdBnxsNbHu3I73RuBpwBat/KHAr7Qm\nzZ+0hP5nwC9OsY+HAi+YtO5T6K6BQlcTO6xNvxg4pyXGcZ/j/5vq/AC7AVetJJ4nTYrnJXS1bWna\nmQClSarqvqr6J+A+uoSwps6kqyntVlXbAR8EMvkwk+bPoPuwfwZwR1V9a8w4/wa4C3hNK74OuKaq\nth95bFtVz5liF9cBH5u07jZVdWJbfh4wJ8k+dInwzJFtx3mOK3Md8PCVlH91Ujyzq+rVY+5XWiMm\nQGmSdA4BdgD+Yy12sS1wc1XdlWQ/utrTKrWEdz/wXlZf+5vsROBPk2wNfAe4Lcmbkzyo3XCyd5J9\np9ju48DvJnlWW2/rdoPPri2me4HP0DUF70iXENf6OY74EPCOJHu2c/34JDsBn6drCn5Zki3aY9+R\na4fStDIBSit8LsntwE+BvwQOr6qla7Gf1wAnJLkNOA749JjbfRR4HF1iWhNfAG4B/rCq7qO7lrYP\ncA3wI7qEs93kjarqOuAQuibSm+hqYG/igZ8LZwIHAJ+Z1OS7ts8R4G/a+l+kO9cfBh5UVbcBB9Jd\nr/xvuibUdwFbrcG+pbHFAXGlDUOSlwPzq2ptml0lrSFrgNIGoP304jXAqTMdizQUJkBphrXf6N1E\n1/vMmatZXdI0sQlUkjRI1gAlSYNkApQkDdLa9kS/wdl5551r7ty5Mx2GJGkDsmTJkh9V1Zyplm0y\nCXDu3LksXrx49StKkgYjyfdXtswmUEnSIJkAJUmD1GsCTHJQG9RyWZJjp1h+ZJLLklyc5N9Hx/1K\n8pa23ZVTjWUmSdK66C0BJpkFnAQ8m25k7cOmGNjyzKp6XFXtA7ybro9A2nqHAo8FDqIbFHNWX7FK\nkoanzxrgfsCyqrq6Dex5Fl3Huz9TVT8dmd2GFUPEHAKcVVV3V9U1dKNd79djrJKkgenzLtBdGBn0\nE7geeNLklZL8MXA0sCXw2yPbXjhp2136CVOSNEQzfhNMVZ1UVQ8H3gz8+Zpsm2R+ksVJFt900039\nBChJ2iT1mQBvAHYbmd+1la3MWcDz1mTbqjq1quZV1bw5c6b8naMkSVPqMwEuAvZMskeSLelualkw\nukKSPUdmDwa+16YXAIcm2SrJHsCedCNdS5I0LXq7BlhVy5McBZwLzAJOq6qlSU4AFlfVAuCoJAcA\n99KNaH1423Zpkk8DVwDLgT9uI11LkjQtNpnhkObNm1d2hSZJGpVkSVXNm2rZJtMX6HSZe+wXZjqE\njca1Jx480yFI0lqb8btAJUmaCSZASdIgmQAlSYNkApQkDZIJUJI0SCZASdIgmQAlSYNkApQkDZIJ\nUJI0SCZASdIgmQAlSYNkApQkDZIJUJI0SCZASdIgmQAlSYNkApQkDZIJUJI0SI4Irw3C3GO/MNMh\nbBSuPfHgmQ5B2mRYA5QkDZIJUJI0SCZASdIgmQAlSYNkApQkDZIJUJI0SCZASdIgmQAlSYNkApQk\nDZIJUJI0SCZASdIgmQAlSYNkApQkDZIJUJI0SCZASdIgmQAlSYPUawJMclCSK5MsS3LsFMuPTnJF\nkkuTfDnJQ0eW3Zfk4vZY0GeckqTh6W1E+CSzgJOAZwLXA4uSLKiqK0ZWuwiYV1V3JHk18G7gRW3Z\nnVW1T1/xSZKGrc8a4H7Asqq6uqruAc4CDhldoaq+UlV3tNkLgV17jEeSpJ/pMwHuAlw3Mn99K1uZ\nI4B/HZnfOsniJBcmeV4fAUqShqu3JtA1keSlwDzgqSPFD62qG5I8DDg/yWVVddWk7eYD8wF23333\n9RavJGnj12cN8AZgt5H5XVvZAyQ5AHgr8NyqunuivKpuaH+vBi4AfnXytlV1alXNq6p5c+bMmd7o\nJUmbtD4T4CJgzyR7JNkSOBR4wN2cSX4VOIUu+f1wpHyHJFu16Z2BJwOjN89IkrROemsCrarlSY4C\nzgVmAadV1dIkJwCLq2oB8B5gNvCZJAA/qKrnAo8BTklyP12SPnHS3aOSJK2TXq8BVtVCYOGksuNG\npg9YyXbfBB7XZ2ySpGGzJxhJ0iCZACVJg2QClCQNkglQkjRIJkBJ0iCZACVJg2QClCQNkglQkjRI\nJkBJ0iCZACVJg2QClCQNkglQkjRIJkBJ0iCZACVJg2QClCQNkglQkjRIJkBJ0iCZACVJg2QClCQN\nkglQkjRIJkBJ0iCZACVJg2QClCQNkglQkjRIJkBJ0iCZACVJg7T5TAcgaWbMPfYLMx3CRuPaEw+e\n6RDUA2uAkqRBMgFKkgbJBChJGiQToCRpkEyAkqRBMgFKkgbJBChJGiQToCRpkHpNgEkOSnJlkmVJ\njp1i+dFJrkhyaZIvJ3noyLLDk3yvPQ7vM05J0vD01hNMklnAScAzgeuBRUkWVNUVI6tdBMyrqjuS\nvBp4N/CiJDsCfwHMAwpY0ra9pa94JWl9sAee8ayP3nf6rAHuByyrqqur6h7gLOCQ0RWq6itVdUeb\nvRDYtU0/Czivqm5uSe884KAeY5UkDUyfCXAX4LqR+etb2cocAfzrWm4rSdIa2SA6w07yUrrmzqeu\n4XbzgfkAu+++ew+RSZI2VX3WAG8AdhuZ37WVPUCSA4C3As+tqrvXZNuqOrWq5lXVvDlz5kxb4JKk\nTV+fCXARsGeSPZJsCRwKLBhdIcmvAqfQJb8fjiw6FzgwyQ5JdgAObGWSJE2L3ppAq2p5kqPoEtcs\n4LSqWprkBGBxVS0A3gPMBj6TBOAHVfXcqro5yTvokijACVV1c1+xSpKGZ40SYJLNgNlV9dNx1q+q\nhcDCSWXHjUwfsIptTwNOW5P4JEka12qbQJOcmeQhSbYBLgeuSPKm/kOTJKk/41wD3KvV+J5H9zOF\nPYCX9RqVJEk9GycBbpFkC7oEuKCq7qXrnUWSpI3WOAnwFOBaYBvga62/zrGuAUqStKFa7U0wVfV+\n4P0jRd9P8vT+QpIkqX/j3ATzJ+0mmCT5cJLvAr+9HmKTJKk34zSB/kG7CeZAYAe6G2BO7DUqSZJ6\nNk4CTPv7HOBjVbV0pEySpI3SOAlwSZIv0iXAc5NsC9zfb1iSJPVrnJ5gjgD2Aa5uA9fuBLyy37Ak\nSerXOHeB3p9kV+DFrb/Or1bV53qPTJKkHo1zF+iJwJ8AV7TH65L8Vd+BSZLUp3GaQJ8D7FNV9wMk\nOQO4CPizPgOTJKlP444HuP3I9HZ9BCJJ0vo0Tg3w/wIXJfkK3c8f9geO7TUqSZJ6Ns5NMJ9McgGw\nbyt6M/2OJC9JUu/GGhC3qm4EFkzMJ/kBsHtfQUmS1Le1rcnZE4wkaaO2tgnQ8QAlSRu1lTaBJvkA\nUye68MC7QiVJ2uis6hrg4rVcJknSBm+lCbCqzlifgUiStD75cwZJ0iCZACVJg2QClCQN0mp/CJ/k\n/VMU3wosrqp/mf6QJEnq3zg1wK3pBsT9Xns8HtgVOCLJ+3qMTZKk3ozTFdrjgSdX1X0ASf4B+Drw\nFOCyHmOTJKk349QAdwBmj8xvA+zYEuLdvUQlSVLPxqkBvhu4uI0IMTEc0l8l2Qb4Uo+xSZLUm3GG\nQ/pwkoXAfq3oz6rqv9v0m3qLTJKkHo37M4jNgJuAW4BHJNm/v5AkSerfOD+DeBfwImApcH8rLuBr\nPcYlSVKvxrkG+DzgUVXlDS+SpE3GOE2gVwNb9B2IJEnr0zgJ8A66u0BPSfL+icc4O09yUJIrkyxL\ncuwUy/dP8t0ky5M8f9Ky+5Jc3B4Lxns6kiSNZ5wm0AXtsUaSzAJOAp4JXA8sSrKgqq4YWe0HwCuA\nY6bYxZ1Vtc+aHleSpHGM8zOItR0XcD9gWVVdDZDkLOAQ4GcJsKqubcvun2oHkiT1ZaUJMMmnq+qF\nSS6ju+vzAarq8avZ9y7AdSPz1wNPWoPYtk6yGFgOnFhV56zBtpIkrdKqaoB/0v7+zvoIZAoPraob\nkjwMOD/JZVV11egKSeYD8wF23333mYhRkrSRWulNMFV1Y/v7/ar6Pt2P4G8beazODcBuI/O7trKx\nVNUN7e/VwAXAr06xzqlVNa+q5s2ZM2fcXUuStPq7QJP8UZL/B1wKLGmPxWPsexGwZ5I9kmwJHMqY\nN9Mk2SHJVm16Z+DJjFw7lCRpXY1zF+gxwN5V9aM12XFVLU9yFHAuMAs4raqWJjmBbjDdBUn2Bf6Z\nbsSJ303y9qp6LPAY4JR2c8xmdNcATYCSpGkzTgK8iu63gGusqhYCCyeVHTcyvYiuaXTydt8EHrc2\nx5QkaRzjJMC3AN9M8m1Gxv+rqtf1FpUkST0bJwGeApxPN/q7v9eTJG0SxkmAW1TV0b1HIknSejRO\nX6D/mmR+kl9OsuPEo/fIJEnq0Tg1wMPa37eMlBXwsOkPR5Kk9WOcvkD3WB+BSJK0Po0zIvws4GBg\n7uj6VfU3/YUlSVK/xmkC/RxwF94FKknahIyTAHcdY+QHSZI2KuPeBXpg75FIkrQejVMDvBD45ySb\nAfcCAaqqHtJrZJIk9WicBPg3wG8Al1XVzw2MK0nSxmicJtDrgMtNfpKkTck4NcCrgQuS/CsP7Azb\nn0FIkjZa4yTAa9pjy/aQJGmjN05PMG9fH4FIkrQ+jdMTzBzgT4HHAltPlFfVb/cYlyRJvRrnJphP\nAP8J7AG8HbgWWNRjTJIk9W6cBLhTVX0YuLeqvlpVfwBY+5MkbdTGuQnm3vb3xiQHA/8NOB6gJGmj\nNk4CfGeS7YA3Ah8AHgK8odeoJEnq2Th3gX6+Td4KPL3fcCRJWj9WmgCTfIBu5PcpVdXreolIkqT1\nYFU1wMUj028H/qLnWCRJWm9WmgCr6oyJ6SSvH52XJGljN87PIGAVTaGSJG2Mxk2AkiRtUlZ1E8xt\nrKj5PTjJTycW4YC4kqSN3KquAW67PgORJGl9sglUkjRIJkBJ0iCZACVJg2QClCQNkglQkjRIJkBJ\n0iD1mgCTHJTkyiTLkhw7xfL9k3w3yfIkz5+07PAk32uPw/uMU5I0PL0lwCSzgJOAZwN7AYcl2WvS\naj8AXgGcOWnbHek6334SsB/wF0l26CtWSdLw9FkD3A9YVlVXV9U9wFnAIaMrVNW1VXUpcP+kbZ8F\nnFdVN1fVLcB5wEE9xipJGpg+E+AuwHUj89e3sr63lSRptTbqm2CSzE+yOMnim266aabDkSRtRPpM\ngDcAu43M79rKpm3bqjq1quZV1bw5c+asdaCSpOHpMwEuAvZMskeSLYFDgQVjbnsucGCSHdrNLwe2\nMkmSpkVvCbCqlgNH0SWu/wA+XVVLk5yQ5LkASfZNcj3wAuCUJEvbtjcD76BLoouAE1qZJEnTYqXD\nIU2HqloILJxUdtzI9CK65s2ptj0NOK3P+CRJw7VR3wQjSdLaMgFKkgbJBChJGiQToCRpkEyAkqRB\nMgFKkgbJBChJGiQToCRpkEyAkqRBMgFKkgbJBChJGiQToCRpkEyAkqRBMgFKkgbJBChJGiQToCRp\nkEyAkqRBMgFKkgbJBChJGiQToCRpkEyAkqRBMgFKkgbJBChJGiQToCRpkEyAkqRBMgFKkgbJBChJ\nGiQToCRpkEyAkqRBMgFKkgbJBChJGiQToCRpkEyAkqRBMgFKkgbJBChJGqReE2CSg5JcmWRZkmOn\nWL5Vkk+15d9OMreVz01yZ5KL2+ODfcYpSRqezfvacZJZwEnAM4HrgUVJFlTVFSOrHQHcUlWPSHIo\n8C7gRW3ZVVW1T1/xSZKGrc8a4H7Asqq6uqruAc4CDpm0ziHAGW36s8AzkqTHmCRJAvpNgLsA143M\nX9/KplynqpYDtwI7tWV7JLkoyVeT/FaPcUqSBqi3JtB1dCOwe1X9OMkTgXOSPLaqfjq6UpL5wHyA\n3XfffQbClCRtrPqsAd4A7DYyv2srm3KdJJsD2wE/rqq7q+rHAFW1BLgKeOTkA1TVqVU1r6rmzZkz\np4enIEnaVPWZABcBeybZI8mWwKHAgknrLAAOb9PPB86vqkoyp91EQ5KHAXsCV/cYqyRpYHprAq2q\n5UmOAs4FZgGnVdXSJCcAi6tqAfBh4GNJlgE30yVJgP2BE5LcC9wPHFlVN/cVqyRpeHq9BlhVC4GF\nk8qOG5m+C3jBFNudDZzdZ2ySpGGzJxhJ0iCZACVJg2QClCQNkglQkjRIJkBJ0iCZACVJg2QClCQN\nkglQkjRIJkBJ0iCZACVJg2QClCQNkglQkjRIJkBJ0iCZACVJg2QClCQNkglQkjRIJkBJ0iCZACVJ\ng2QClCQNkglQkjRIJkBJ0iCZACVJg2QClCQNkglQkjRIJkBJ0iCZACVJg2QClCQNkglQkjRIJkBJ\n0iCZACVJg2QClCQNkglQkjRIJkBJ0iCZACVJg2QClCQNUq8JMMlBSa5MsizJsVMs3yrJp9rybyeZ\nO7LsLa38yiTP6jNOSdLw9JYAk8wCTgKeDewFHJZkr0mrHQHcUlWPAP4WeFfbdi/gUOCxwEHAyW1/\nkiRNiz5rgPsBy6rq6qq6BzgLOGTSOocAZ7TpzwLPSJJWflZV3V1V1wDL2v4kSZoWfSbAXYDrRuav\nb2VTrlNVy4FbgZ3G3FaSpLW2+UwHsC6SzAfmt9nbk1w5k/H0bGfgRzMdxKi8a6Yj6J3nfP3b4M45\neN5nwjSe84eubEGfCfAGYLeR+V1b2VTrXJ9kc2A74MdjbktVnQqcOo0xb7CSLK6qeTMdx5B4ztc/\nz/nMGOp577MJdBGwZ5I9kmxJd1PLgknrLAAOb9PPB86vqmrlh7a7RPcA9gS+02OskqSB6a0GWFXL\nkxwFnAvMAk6rqqVJTgAWV9UC4MPAx5IsA26mS5K09T4NXAEsB/64qu7rK1ZJ0vCkq3BpQ5dkfmvy\n1XriOV//POczY6jn3QQoSRoku0KTJA2SCXAtJKkk7x2ZPybJ8avZ5rlTdQc35vGOT3JDkouT/GeS\nf0iyyf/vktzXnvPSJJckeePaPu8kJyQ5YBXLj0zy8rXY77NajBcnub113Xdxko+uTZwzYeQ8X57k\nc0m2n6b9zk1y+TTt6/Qk14yc69dNx35XcqynJfnNtdz29mk4/q8k+ewqlm+f5DXjrt/WuaC9Ni9J\nsijJPusa53Ra3fuzt+PaBLrmktwF3AjsW1U/SnIMMLuqju/peMcDt1fVX7cE8DXgbVX1lT6Ot6FI\ncntVzW7TvwCcCXyjqv5iZiObWpILgGOqavEUyzZvnT1scCad5zOA/6qqv5yG/c4FPl9Ve0/Dvk5v\n+1rlB/1Ktp21JjfRjb7f1uJYPzuXfVmb8zr62kzySuDFVfXMaYhlg31dj2OTr0X0ZDnd7w/fMHlB\nkt9tHXtflORLSX6xlb8iyd8n2S7J9ydqMkm2SXJdki2SPDzJvyVZkuTrSR49xbG3BLYGbmnb/2H7\nRndJkrOTPDjJtu3b8hZtnYdMzK/sGEle0GoAlyT5Wi9nbR1U1Q/pOj04Kp1ZSd7TnvulSf5oYt0k\nb05yWXsuJ7ay05M8v02fmOSKtt1ft7Lj2xcZkuyT5MK2/J+T7NDKL0jyriTfSfJfSX5rVTEneVWS\nc5J8he5uaJIc27a/NMlxI+se3sovTnJyZq6G/y1ar0tJZif5cpLvtvN5SCufm+Q/kvxjutr5F5M8\nqC17YjvvlwB/PLHTJFsn+Ujbz0VJnt7KX9HO0XlJrk1yVJKj2zoXJtlxVcEmOazt8/JkxU+n09XG\n39vi+I0W11fb6/7cJL/c1nvdyGvhrHTJ5UjgDe1/scr/8Tja+Tq/HePLSXZv5Q9vz/GyJO9Mqz1m\npOac5LEjr4tLk+wJnAg8vJW9Z9L6s5L8dTsflyZ57RQh/ex/3LY5MMm32v/5M0kmvgw9J12L05Ik\n70/y+VZ+fJKPJfkG3V38U74Xk/xykq9lRevCb7V1T2/zlyV5Q1t39P35jPb/vyzJaUm2auXXJnn7\nyOtxqs/HNVNVPtbwAdwOPAS4lu7H+8cAx7dlO7CiZv0q4L1t+hXA37fpfwGe3qZfBHyoTX8Z2LNN\nP4nud5EAx9N1BHAxXeI7cySWnUam3wm8tk1/BHhem54/EsfKjnEZsEub3n6mz/HEeZ6i7CfAL7bn\n9OetbCtgMbAHXefr3wQe3Jbt2P6eTvdb052AK0f+R9uPnONj2vSlwFPb9AnA+9r0BSPn8TnAlybF\ndgEwb2T+VcD3gR1GtjkZCN2Xz38DfhPYGzgH2LytdyrdN/T1ep7pfq70GeCgNr858JA2vTNdn7wB\n5tJ9CdynLfs08NKRc7d/m34PcHmbfiPdT6EAHg38gO6L3CvafrcF5tB1h3hkW+9vgdeP/P+uoXsP\nXAw8DviVtp85LdbzWfGaL+CFbXqL9pqYM/Kem4jlv4GtVvZamKbX7OeAw9v0HwDntOnPA4e16SNH\n/g9zR87bB4CXtOktgQeNLp9i/VfT9as88VqaeP1fQHttAq8H/mrk//o1YJs2/2bguPa/uQ7Yo5V/\nkq7WOXF+lgAPavMrey++EXjryGtrW+CJwHkjsU+c89Pp3p8Tx31kK//oyGvgWlZ8vr2G9rm5Lo+N\nuiu0mVRVP013ned1wJ0ji3YFPtW+YW5J96ad7FN0b8Kv0P328eT2res3gc8kmVhvq5Ft/ra6JtAt\ngM8mObSqzgL2TvJOYHtgNq2mAXwI+FO6D9ZXAn+4mmN8Azg93e8v/2mNT8j6dyDw+IlvjXRfRPYE\nDgA+UlV3AFTVzZO2uxW4C/hw+0b7+dGFSbaje1N+tRWdQZcUJkycmyV0Hzyr88WqumUk5mcDF7X5\n2cAj6f53+wKL2//lQTywL9y+PSjJxXS1gv8AzmvlAf4qyf7A/W35L7Zl11TVxW16CTA33bXD7atq\nogXhY3TPF+ApdB/mVNV/Jvk+3XMH+EpV3QbcluRWuoQB3Zeyx4/E+aYaaQJNVyO9oKpuavOfAPan\ne83fB5zdVn0U3ZeM89r5nUV3CQO6hP2JJOe07frwG8Dvt+mPAe8eKX9emz4TmKrJ9VvAW5PsCvxT\nVX1v5L07lQOAD1Zrlpz0+v9Euk5JZgMT1wB/nW60nm+0/W7Zjvlo4OrqBiOALgHOH9nXgqqa+Nxb\n2XtxEXBa+8w6p6ouTnI18LAkHwC+AHxxUvyPontt/VebP4OuJeF9bX70/ff7rCObQNfN++iGdNpm\npOwDdDW9xwF/RPeNZrIFwEGteeeJdN9cNwN+UlX7jDweM3nDqrqXruawfys6HTiqHe/tE8erqm/Q\nfSg9DZhVVZev6hhVdSTw53Rd0C1JstNan5WeJHkY3QfbD+k+nF878jz2qKrJb6af0z4Y9qP7lvw7\ndOdyTdzd/t7HeB1J/O/IdIB3jsT8iKo6vZWfNlL+qKp6xxrGtS7urKp96PpMDCuaLl9CV7t6Ylv+\nP6x4Pd89sv2452JlRvd1/8j8/euw37tqxXW/AEtHzu/jqurAtuxgumHbfg1YlK5Lxg1GVZ0JPJfu\nS/bCJL+9Drt7CfAwuqTygVYWuhrZxLnZq6qOGGNfk1/XP/debF+E9qdrvTo9ycvbl8En0NVIj6T7\nor4m1vT9t0omwHXQvl19mi4JTtiOFf2WHv5zG3Xb3U737ejv6JoV7quqnwLXJHkBQDpPmLxtuq9p\nTwauakXbAje2b1kvmbT6R+m+WX6kHXelx0jy8Kr6dlUdB9zEA/tinXFJ5gAfpPtyUXQ13VdnxXXO\nRybZhq728sokD27lO07az2xgu6paSHcN9wHnuKpuBW7Jims/LwO+yvQ4FziixUmSXZPsDHwJeGGb\nJslOadeJ1qdWa34d8Mas6Jv3h1V1b7prdivtVLht/xPgJ0me0opGX49fn5hP8khgd7qm6HXxHeCp\nSXZON17oYUz9v7oSmJPkN9rxt0h3bW0zYLfqbiZ7M93znQ3cRve+mi7fpPVyRXcOvt6mLwT+T5s+\ndPJGLdaH0dXE3k936eTxqxRsEgUAAAQtSURBVInvPOCPJhL55Nd/e++8Dfj1dg3tQuDJSR7R1t+m\n/X+upKupzW2bvmgVz2/K92KShwL/U1X/SJfofq29xjerqrPpvnD/2qR9XUn3xf0RbX46338/xwS4\n7t5L144+4Xi6JsYlrLp39U8BL21/J7yE7gPyEmApDxw/8Q2tmepyuiack1v524Bv0zVh/uekY3yC\n7prkJ8c4xnvaheXL6d6wl6wi9vXlQe0C+lK6JPFFuloudG+oK4DvtphPobvu8W90NezF7XwdM2mf\n2wKfT3Ip8O/A0VMc93C683EpXVPRCdPxZFrS/SxwYZLL6L48za6qy9rz+lI75hdZ0dS4XlXVRXTN\ngofRvX7mtVhfzs+/vqbySuCkdu5H2+pOBjZr+/oU8IqqunuqHaxBrDcCx9JdSrgEWFJV/zLFevfQ\nXV96V3vdX0x3KWAW8PEW00XA+1sS/xzwe1m7m2AenOT6kcfRwGvpvpRdSveB/idt3dcDR7fyR9A1\nz0/2QuDydj73Bj5aVT+ma7K8PMl7Jq3/Ibrrope25/riKc7HnXSfW29qzcevAD7Z4vgW8Oi2zmuA\nf2ufZbetJL6JY/7cexF4GnBJkovoEujf0TWjX9Cez8eBt0yK7S6619Bn2v/lfrovvr3wZxCbsNYm\nf0hVvWymY5H0QK2V4s6qqiSH0t0QM3nQ8BmTZHZV3d5anU4CvldVfzvTcU2nDaq9W9OnXWR+Nt2d\nh5I2PE8E/r4lmJ/Q3SG6IfnDJIfT3RhzEV3NbpNiDVCSNEheA5QkDZIJUJI0SCZASdIgmQClGZTk\nl9L1QXlVuj4XF7bfUU3LKArtGD/raT9df4xL2y3+u2Q1owhImzJvgpFmSLv775vAGVX1wVb2BLp+\nZv+hpmEUhSmO+UHg36vq42ux7Ubd8780mTVAaeY8Hbh3IvkBVNUljPQDmq6n/6+n6wH/u2nj1GUt\netpP8iq6H1a/I8kn8vOjCEzVo//T2vEX0P3YWdpk+DtAaebsTdep76r8EHhmVd2VbiicTwLz6Hr4\nOLeq/rJ1A/Zgul5rdpmoOWbSwLZV9aHWTdnnq+qzI91cQded361VtW+64We+kWSib9VfA/Ye6RhZ\n2iSYAKUN2xZ0P5beh64D4IkRFNamp/1VWVmP/vcA3zH5aVNkE6g0c5bS9QayKm+gG4XhCXQ1vy0B\neuhpf1Wja/zvqjaUNlYmQGnmnA9sleRn46wleTwPHIljO+DGqrqfriPlWW29telpf1VWNrqGtMmy\nCVSaIa0T5N8D3pfkzXQD9V5LN0rAhJOBs5O8nG7swona2NOANyW5F7idbrSGXYCPpBvmByb1tL8a\nH6Ib4Pe77e7Um1gxWKu0SfJnEJKkQbIJVJI0SCZASdIgmQAlSYNkApQkDZIJUJI0SCZASdIgmQAl\nSYNkApQkDdL/BwOjCT0Z8o7AAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "yLabels = [\"NaiveBayes\",\"DecisionTree\",\"RandomForest\",\"LogisticRegression\"]\n",
    "plt.subplots(1,1,figsize=(7,5))\n",
    "fig1 = plt.bar(yLabels,binaryRelevanceList)\n",
    "plt.ylabel('Hamming Loss')\n",
    "plt.xlabel('Classifier')\n",
    "plt.title(\"Binary Relevance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "LP8U8DyW1omf",
    "outputId": "bca3d646-d12d-4779-96c2-c28d321d0287"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Binary Relevance with under sampling')"
      ]
     },
     "execution_count": 66,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbkAAAFNCAYAAACdVxEnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmZkVy0nOzMyK9f8BSyH88/xDtKgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplots(1,1,figsize=(7,5))\n",
    "fig2 = plt.bar(yLabels,binaryRelevanceWithUndersamplingList)\n",
    "plt.ylabel('Hamming Loss')\n",
    "plt.xlabel('Classifier')\n",
    "plt.title(\"Binary Relevance with under sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "LlhN4J9Q1omh",
    "outputId": "bd9bdbdb-6783-45cd-f82a-95dbe1e2965c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Chain Classification')"
      ]
     },
     "execution_count": 67,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcAAAAFNCAYAAACXC791AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplots(1,1,figsize=(7,5))\n",
    "fig3 = plt.bar(yLabels,chainClassificationList)\n",
    "plt.ylabel('Hamming Loss')\n",
    "plt.xlabel('Classifier')\n",
    "plt.title(\"Chain Classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bIukkXjx1omi"
   },
   "source": [
    "## Task 6: Reflect on the Performance of the Different Models Evaluated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xki5bLXb1omj"
   },
   "source": [
    "A macro-average computes each of the metric independently for each class and then take the average, hence treating all classes equally.\n",
    "**********************************************************************************************************\n",
    "The Hamming Loss (HL) is the fraction of the wrong labels to the total number of labels. Hence, for the binary case (imbalanced or not), HL = 1 - Accuracy. It gives each label equal weighting.\n",
    "**********************************************************************************************************\n",
    "Thus we have chosen Hamming Loss as evaluating factor for classifiers.<br>\n",
    "Below is the table detailing values of Hamming Loss and Macro Avg f1-Score for different algorithm and classifier combinations:\n",
    "\n",
    "|Algorithm|Classifer|Hamming Loss|Macro Avg f1-Score|\n",
    "| --- | --- | --- | --- |\n",
    "|BinaryRelavanceAlgorithm|NaÃ¯ve Bayes|0.294|0.46|\n",
    "|BinaryRelavanceAlgorithm|Decision Tree|0.274|0.41|\n",
    "|BinaryRelavanceAlgorithm|RandomForest|0.214|0.25|\n",
    "|BinaryRelavanceAlgorithm|Logistic Regression|<b>0.203</b>|0.35|\n",
    "| --- | --- | --- | --- |\n",
    "|BinaryRelavanceAlgorithm with Under Sampling|NaÃ¯ve Bayes|0.374|0.45|\n",
    "|BinaryRelavanceAlgorithm with Under Sampling|Decision Tree|0.413|0.41|\n",
    "|BinaryRelavanceAlgorithm with Under Sampling|RandomForest|0.438|0.47|\n",
    "|BinaryRelavanceAlgorithm with Under Sampling|Logistic Regression|<b>0.360</b>|0.46|\n",
    "| --- | --- | --- | --- |\n",
    "|ClassifierChainsAlgorithm|NaÃ¯ve Bayes|0.314|0.45|\n",
    "|ClassifierChainsAlgorithm|Decision Tree|0.269|0.41|\n",
    "|ClassifierChainsAlgorithm|RandomForest|0.214|0.25|\n",
    "|ClassifierChainsAlgorithm|Logistic Regression|<b>0.214</b>|0.40|\n",
    "\n",
    "From the table it is evident that <b>Logistic Regression</b> works the best in all the three cases. That is exactly the same result we observed when we evaluated using Grid Search CV.\n",
    "\n",
    "We expected better results (i.e. lower values of hamming loss) both in the cases of undersampling and using the chain classifier, but that was not the case. For binary relevance with undersampling, we suspect the reason to be that when undersampling was not done, there might be more records of a class label in the training set and the same class label records were also greater in number in the test set as well. <br/>\n",
    "While, in the case of chain classification, the class labels might not have any dependencies between them, so combining the labels along with the features might have reduced the prediction accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HRtya1yQ1omk"
   },
   "source": [
    "## Exporting the note-book to html format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "A0ZmjOgg1omk",
    "outputId": "eafc799a-ae1e-44b5-ac94-59a43bfe6249"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Assignment_1_Shaurya(19200891)_Susmitha(19200996).ipynb to html\n",
      "[NbConvertApp] Writing 500006 bytes to Assignment_1_Shaurya(19200891)_Susmitha(19200996).html\n"
     ]
    }
   ],
   "source": [
    "! jupyter nbconvert --to html Assignment_1_Shaurya(19200891)_Susmitha(19200996).ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Sfyw0GR1omm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Assignment1_MultiLabelClassification_Solution_updated.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
